{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217c8baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.55.2\n",
      "PyTorch version: 2.7.1+cu126\n",
      "09/25/2025 20:23:58 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/25/2025 20:23:58 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/25/2025 20:23:58 - INFO - __main__ - Original class distribution: {0: 23373, 1: 676, 2: 122, 3: 4227, 4: 2331, 5: 8212}\n",
      "09/25/2025 20:23:58 - INFO - __main__ - Augmented label 2 (Sexism) from 122 to 150 samples\n",
      "09/25/2025 20:23:58 - INFO - __main__ - Training data: 38941 -> 38969 samples\n",
      "09/25/2025 20:23:58 - INFO - __main__ - Final train label distribution:\n",
      "label\n",
      "0    23373\n",
      "1      676\n",
      "2      150\n",
      "3     4227\n",
      "4     2331\n",
      "5     8212\n",
      "Name: count, dtype: int64\n",
      "09/25/2025 20:23:58 - INFO - __main__ - Validation label distribution:\n",
      "label\n",
      "0    1451\n",
      "1      38\n",
      "2      11\n",
      "3     291\n",
      "4     157\n",
      "5     564\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 38969/38969 [00:03<00:00, 10227.52 examples/s]\n",
      "Map: 100%|██████████| 2512/2512 [00:00<00:00, 10989.95 examples/s]\n",
      "Map: 100%|██████████| 10200/10200 [00:00<00:00, 10921.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 20:24:04 - INFO - __main__ - Conservative class weights: [0.5, 1.288678765296936, 2.024423122406006, 0.7435651421546936, 0.8889268040657043, 0.6092478632926941]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 20:24:06 - INFO - __main__ - Model loaded on cuda\n",
      "09/25/2025 20:24:06 - INFO - __main__ - Total parameters: 113,310,471\n",
      "09/25/2025 20:24:06 - INFO - __main__ - Trainable parameters: 113,310,471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 20:24:06 - INFO - __main__ - Starting conservative training approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Local\\Temp\\ipykernel_7044\\3436471014.py:363: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5800' max='7308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5800/7308 39:06 < 10:10, 2.47 it/s, Epoch 4/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Class 0 None</th>\n",
       "      <th>F1 Class 1 Religious Hate</th>\n",
       "      <th>F1 Class 2 Sexism</th>\n",
       "      <th>F1 Class 3 Political Hate</th>\n",
       "      <th>F1 Class 4 Profane</th>\n",
       "      <th>F1 Class 5 Abusive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.270800</td>\n",
       "      <td>1.205767</td>\n",
       "      <td>0.614650</td>\n",
       "      <td>0.266641</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.767253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.345992</td>\n",
       "      <td>0.097710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.163200</td>\n",
       "      <td>1.080684</td>\n",
       "      <td>0.634156</td>\n",
       "      <td>0.323780</td>\n",
       "      <td>0.573902</td>\n",
       "      <td>0.783874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.522911</td>\n",
       "      <td>0.135894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.122400</td>\n",
       "      <td>1.022637</td>\n",
       "      <td>0.657643</td>\n",
       "      <td>0.355776</td>\n",
       "      <td>0.617633</td>\n",
       "      <td>0.795517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.563574</td>\n",
       "      <td>0.304136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.061800</td>\n",
       "      <td>1.032115</td>\n",
       "      <td>0.597930</td>\n",
       "      <td>0.373492</td>\n",
       "      <td>0.594128</td>\n",
       "      <td>0.754704</td>\n",
       "      <td>0.199234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489112</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.300248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.013800</td>\n",
       "      <td>1.002723</td>\n",
       "      <td>0.633758</td>\n",
       "      <td>0.367911</td>\n",
       "      <td>0.581086</td>\n",
       "      <td>0.776528</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.513644</td>\n",
       "      <td>0.550296</td>\n",
       "      <td>0.158046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.003100</td>\n",
       "      <td>0.953975</td>\n",
       "      <td>0.654857</td>\n",
       "      <td>0.420743</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>0.779494</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531002</td>\n",
       "      <td>0.642140</td>\n",
       "      <td>0.386640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.956720</td>\n",
       "      <td>0.680732</td>\n",
       "      <td>0.414279</td>\n",
       "      <td>0.638391</td>\n",
       "      <td>0.802750</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.664516</td>\n",
       "      <td>0.308290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.934100</td>\n",
       "      <td>0.974170</td>\n",
       "      <td>0.675557</td>\n",
       "      <td>0.403925</td>\n",
       "      <td>0.638700</td>\n",
       "      <td>0.804023</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.641304</td>\n",
       "      <td>0.341523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.902800</td>\n",
       "      <td>0.874360</td>\n",
       "      <td>0.673567</td>\n",
       "      <td>0.457230</td>\n",
       "      <td>0.662095</td>\n",
       "      <td>0.794336</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.410835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.904690</td>\n",
       "      <td>0.640924</td>\n",
       "      <td>0.450463</td>\n",
       "      <td>0.650627</td>\n",
       "      <td>0.761095</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522979</td>\n",
       "      <td>0.621176</td>\n",
       "      <td>0.475303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>0.901678</td>\n",
       "      <td>0.680334</td>\n",
       "      <td>0.443064</td>\n",
       "      <td>0.660890</td>\n",
       "      <td>0.796910</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531343</td>\n",
       "      <td>0.703226</td>\n",
       "      <td>0.408726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.898900</td>\n",
       "      <td>0.868646</td>\n",
       "      <td>0.673965</td>\n",
       "      <td>0.461502</td>\n",
       "      <td>0.650229</td>\n",
       "      <td>0.792656</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548760</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.356704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.821400</td>\n",
       "      <td>0.871699</td>\n",
       "      <td>0.651672</td>\n",
       "      <td>0.479546</td>\n",
       "      <td>0.663048</td>\n",
       "      <td>0.757974</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509275</td>\n",
       "      <td>0.700315</td>\n",
       "      <td>0.519084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.841200</td>\n",
       "      <td>0.859049</td>\n",
       "      <td>0.682325</td>\n",
       "      <td>0.459680</td>\n",
       "      <td>0.657398</td>\n",
       "      <td>0.798011</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.653199</td>\n",
       "      <td>0.386417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>0.843452</td>\n",
       "      <td>0.679936</td>\n",
       "      <td>0.495139</td>\n",
       "      <td>0.674392</td>\n",
       "      <td>0.796077</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.543720</td>\n",
       "      <td>0.674487</td>\n",
       "      <td>0.460754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.871557</td>\n",
       "      <td>0.667596</td>\n",
       "      <td>0.468592</td>\n",
       "      <td>0.659299</td>\n",
       "      <td>0.785160</td>\n",
       "      <td>0.407080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532374</td>\n",
       "      <td>0.654867</td>\n",
       "      <td>0.432071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.857700</td>\n",
       "      <td>0.868165</td>\n",
       "      <td>0.666799</td>\n",
       "      <td>0.467710</td>\n",
       "      <td>0.661395</td>\n",
       "      <td>0.782816</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531685</td>\n",
       "      <td>0.695402</td>\n",
       "      <td>0.439919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.815400</td>\n",
       "      <td>0.858443</td>\n",
       "      <td>0.656051</td>\n",
       "      <td>0.496592</td>\n",
       "      <td>0.642606</td>\n",
       "      <td>0.779186</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.538799</td>\n",
       "      <td>0.697406</td>\n",
       "      <td>0.357054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.875655</td>\n",
       "      <td>0.695064</td>\n",
       "      <td>0.491754</td>\n",
       "      <td>0.684323</td>\n",
       "      <td>0.805246</td>\n",
       "      <td>0.320988</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.542907</td>\n",
       "      <td>0.697947</td>\n",
       "      <td>0.478170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.841133</td>\n",
       "      <td>0.684713</td>\n",
       "      <td>0.515862</td>\n",
       "      <td>0.686844</td>\n",
       "      <td>0.790261</td>\n",
       "      <td>0.357895</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.694362</td>\n",
       "      <td>0.521817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.736400</td>\n",
       "      <td>0.853075</td>\n",
       "      <td>0.666003</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.663869</td>\n",
       "      <td>0.780216</td>\n",
       "      <td>0.407080</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.549790</td>\n",
       "      <td>0.699140</td>\n",
       "      <td>0.440714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.745100</td>\n",
       "      <td>0.852425</td>\n",
       "      <td>0.668790</td>\n",
       "      <td>0.519928</td>\n",
       "      <td>0.667591</td>\n",
       "      <td>0.782339</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.549849</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.448770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>0.830007</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.536605</td>\n",
       "      <td>0.679220</td>\n",
       "      <td>0.782946</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.556391</td>\n",
       "      <td>0.681672</td>\n",
       "      <td>0.498578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>0.825531</td>\n",
       "      <td>0.684315</td>\n",
       "      <td>0.535247</td>\n",
       "      <td>0.682916</td>\n",
       "      <td>0.798618</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.550143</td>\n",
       "      <td>0.711246</td>\n",
       "      <td>0.470954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.722600</td>\n",
       "      <td>0.847008</td>\n",
       "      <td>0.688694</td>\n",
       "      <td>0.546877</td>\n",
       "      <td>0.691701</td>\n",
       "      <td>0.791477</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.563333</td>\n",
       "      <td>0.670846</td>\n",
       "      <td>0.530909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.710300</td>\n",
       "      <td>0.830377</td>\n",
       "      <td>0.692675</td>\n",
       "      <td>0.526676</td>\n",
       "      <td>0.687659</td>\n",
       "      <td>0.802448</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.699140</td>\n",
       "      <td>0.483871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.847434</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.535218</td>\n",
       "      <td>0.682642</td>\n",
       "      <td>0.781792</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.541245</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>0.520147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>0.852078</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.523878</td>\n",
       "      <td>0.684413</td>\n",
       "      <td>0.799726</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.554331</td>\n",
       "      <td>0.702857</td>\n",
       "      <td>0.477273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.848302</td>\n",
       "      <td>0.668790</td>\n",
       "      <td>0.526832</td>\n",
       "      <td>0.676764</td>\n",
       "      <td>0.768839</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.545151</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.529857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 21:03:24 - INFO - __main__ - Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 21:03:36 - INFO - __main__ - Validation Results:\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_loss: 0.8470\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_micro: 0.6887\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_macro: 0.5469\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_weighted: 0.6917\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_class_0_None: 0.7915\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_class_1_Religious_Hate: 0.4615\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_class_2_Sexism: 0.2632\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_class_3_Political_Hate: 0.5633\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_class_4_Profane: 0.6708\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_f1_class_5_Abusive: 0.5309\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_runtime: 11.7348\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_samples_per_second: 214.0640\n",
      "09/25/2025 21:03:36 - INFO - __main__ - eval_steps_per_second: 6.7320\n",
      "09/25/2025 21:03:48 - INFO - __main__ - \n",
      "Validation Classification Report:\n",
      "09/25/2025 21:03:48 - INFO - __main__ - \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          None     0.8024    0.7808    0.7915      1451\n",
      "Religious Hate     0.3636    0.6316    0.4615        38\n",
      "        Sexism     0.1852    0.4545    0.2632        11\n",
      "Political Hate     0.5469    0.5808    0.5633       291\n",
      "       Profane     0.6605    0.6815    0.6708       157\n",
      "       Abusive     0.5448    0.5177    0.5309       564\n",
      "\n",
      "      accuracy                         0.6887      2512\n",
      "     macro avg     0.5172    0.6078    0.5469      2512\n",
      "  weighted avg     0.6968    0.6887    0.6917      2512\n",
      "\n",
      "09/25/2025 21:03:48 - INFO - __main__ - Generating test predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 21:04:23 - INFO - __main__ - Predictions saved to ./conservative_banglabert_hate_speech/subtask_1A.tsv\n",
      "09/25/2025 21:04:23 - INFO - __main__ - \n",
      "🎯 Final Results:\n",
      "09/25/2025 21:04:23 - INFO - __main__ - F1-micro score: 0.6887\n",
      "09/25/2025 21:04:23 - INFO - __main__ - F1-macro score: 0.5469\n",
      "09/25/2025 21:04:23 - INFO - __main__ - 📈 Need further tuning for better macro-F1\n",
      "09/25/2025 21:04:23 - INFO - __main__ - Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Bangla Hate Speech Classification - Conservative Improvements for F1-Macro\n",
    "# Focus: Balanced approach without over-engineering that hurts minority classes\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim import AdamW\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Dataset paths\n",
    "train_file = 'merged_dataset.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1A_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1A_test.tsv'\n",
    "\n",
    "# Enhanced but conservative text preprocessing\n",
    "def clean_bangla_text(text):\n",
    "    \"\"\"Enhanced preprocessing for Bangla text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Normalize Unicode (important for Bangla)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Clean excessive punctuation\n",
    "    text = re.sub(r'[।!?]{3,}', '।।', text)\n",
    "    \n",
    "    # Remove digits mixed with text\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Labels\n",
    "hate_l2id = {'None': 0, 'Religious Hate': 1, 'Sexism': 2, 'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\n",
    "id2hate = {v: k for k, v in hate_l2id.items()}\n",
    "num_labels = len(hate_l2id)\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_clean_dataset(file_path, is_test=False):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df['text'] = df['text'].apply(clean_bangla_text)\n",
    "    \n",
    "    # Remove empty texts\n",
    "    df = df[df['text'].str.len() > 0]\n",
    "    \n",
    "    if not is_test:\n",
    "        df['label'] = df['label'].map(hate_l2id)\n",
    "        if df['label'].isna().any():\n",
    "            logger.warning(f\"Unmapped labels found, filling with 0\")\n",
    "            df['label'] = df['label'].fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = load_and_clean_dataset(train_file)\n",
    "val_df = load_and_clean_dataset(validation_file)\n",
    "test_df = load_and_clean_dataset(test_file, is_test=True)\n",
    "\n",
    "# CONSERVATIVE data augmentation - only for very small classes\n",
    "def conservative_augmentation(df, min_threshold=200):\n",
    "    \"\"\"Only augment classes that are extremely small\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    class_counts = df['label'].value_counts().sort_index()\n",
    "    logger.info(f\"Original class distribution: {class_counts.to_dict()}\")\n",
    "    \n",
    "    for label in df['label'].unique():\n",
    "        label_data = df[df['label'] == label].copy()\n",
    "        current_count = len(label_data)\n",
    "        \n",
    "        # Only augment if class is very small\n",
    "        if current_count < min_threshold:\n",
    "            needed = min_threshold - current_count\n",
    "            \n",
    "            # Simple duplication with slight variations\n",
    "            for _ in range(needed):\n",
    "                sample = label_data.sample(1).iloc[0]\n",
    "                text = sample['text']\n",
    "                \n",
    "                # Very minimal modifications to avoid introducing noise\n",
    "                if len(text.split()) > 3 and random.random() < 0.3:\n",
    "                    words = text.split()\n",
    "                    # Only shuffle adjacent words occasionally\n",
    "                    if len(words) >= 4:\n",
    "                        idx = random.randint(0, len(words) - 2)\n",
    "                        words[idx], words[idx + 1] = words[idx + 1], words[idx]\n",
    "                        text = ' '.join(words)\n",
    "                \n",
    "                augmented_data.append({\n",
    "                    'text': text,\n",
    "                    'label': label\n",
    "                })\n",
    "            \n",
    "            logger.info(f\"Augmented label {label} ({id2hate[label]}) from {current_count} to {min_threshold} samples\")\n",
    "    \n",
    "    if augmented_data:\n",
    "        augmented_df = pd.DataFrame(augmented_data)\n",
    "        combined_df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "        return combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply very conservative augmentation\n",
    "original_train_size = len(train_df)\n",
    "train_df = conservative_augmentation(train_df, min_threshold=150)  # Very conservative\n",
    "logger.info(f\"Training data: {original_train_size} -> {len(train_df)} samples\")\n",
    "\n",
    "# Log distributions\n",
    "logger.info(\"Final train label distribution:\\n%s\", train_df['label'].value_counts().sort_index())\n",
    "logger.info(\"Validation label distribution:\\n%s\", val_df['label'].value_counts().sort_index())\n",
    "\n",
    "# Convert to datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Model setup\n",
    "model_name = 'csebuetnlp/banglabert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Use original sequence length\n",
    "max_seq_length = 384\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Clean datasets\n",
    "train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names \n",
    "                                            if col not in ['input_ids', 'attention_mask', 'label']])\n",
    "val_dataset = val_dataset.remove_columns([col for col in val_dataset.column_names \n",
    "                                        if col not in ['input_ids', 'attention_mask', 'label']])\n",
    "\n",
    "test_columns_to_keep = ['input_ids', 'attention_mask', 'id']\n",
    "test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names \n",
    "                                          if col not in test_columns_to_keep])\n",
    "\n",
    "# CONSERVATIVE class weights - not too aggressive\n",
    "classes = np.unique(train_df['label'])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_df['label'])\n",
    "\n",
    "# Apply moderate adjustment - much less aggressive than before\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "max_count = class_counts.max()\n",
    "conservative_weights = []\n",
    "\n",
    "for i, count in enumerate(class_counts):\n",
    "    # Much more conservative weighting\n",
    "    weight_multiplier = (max_count / count) ** 0.3  # Much less aggressive than 0.75\n",
    "    conservative_weights.append(weight_multiplier)\n",
    "\n",
    "conservative_weights = np.array(conservative_weights)\n",
    "# Normalize and cap the weights to prevent extreme values\n",
    "conservative_weights = np.clip(conservative_weights / conservative_weights.mean(), 0.5, 3.0)\n",
    "\n",
    "class_weights = torch.tensor(conservative_weights, dtype=torch.float)\n",
    "logger.info(f\"Conservative class weights: {class_weights.tolist()}\")\n",
    "\n",
    "# Improved but not over-engineered model\n",
    "class ImprovedButSimpleModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels, hidden_size=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Base transformer - NO FREEZING (learned this lesson!)\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        # Simple but effective improvements\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Much lower dropout\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        \n",
    "        # Multi-scale CNN - simplified\n",
    "        self.conv1 = nn.Conv1d(hidden_size, 256, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, 256, kernel_size=5, padding=2)\n",
    "        \n",
    "        # Simple Bi-LSTM\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=256 * 2,  # from 2 conv layers\n",
    "            hidden_size=256,\n",
    "            num_layers=1,  # Simpler\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in LSTM\n",
    "        )\n",
    "        \n",
    "        # Simple attention pooling\n",
    "        self.attention = nn.Linear(256 * 2, 1)\n",
    "        \n",
    "        # Simple classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Base transformer output\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout1(sequence_output)\n",
    "        \n",
    "        # Multi-scale CNN\n",
    "        sequence_transposed = sequence_output.permute(0, 2, 1)\n",
    "        \n",
    "        cnn_out1 = torch.relu(self.conv1(sequence_transposed))\n",
    "        cnn_out2 = torch.relu(self.conv2(sequence_transposed))\n",
    "        \n",
    "        # Combine features\n",
    "        combined_cnn = torch.cat([cnn_out1, cnn_out2], dim=1)\n",
    "        combined_cnn = combined_cnn.permute(0, 2, 1)\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        lstm_out, _ = self.bilstm(combined_cnn)\n",
    "        lstm_out = self.dropout2(lstm_out)\n",
    "        \n",
    "        # Attention pooling\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        pooled = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Simple weighted cross-entropy - no complex losses\n",
    "            loss = F.cross_entropy(logits, labels, weight=class_weights.to(logits.device))\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss} if loss is not None else {'logits': logits}\n",
    "\n",
    "# Initialize model\n",
    "model = ImprovedButSimpleModel(model_name, num_labels)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "logger.info(f\"Model loaded on {device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Total parameters: {total_params:,}\")\n",
    "logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Enhanced metrics computation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    f1_weighted = f1_score(labels, preds, average=\"weighted\")\n",
    "    \n",
    "    # Per-class F1 scores\n",
    "    f1_per_class = f1_score(labels, preds, average=None)\n",
    "    \n",
    "    result = {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "    }\n",
    "    \n",
    "    # Add per-class metrics with cleaner names\n",
    "    for i, f1 in enumerate(f1_per_class):\n",
    "        class_name = id2hate[i].replace(' ', '_').replace('/', '_')\n",
    "        result[f\"f1_class_{i}_{class_name}\"] = f1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# More conservative training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./conservative_banglabert_hate_speech\",\n",
    "    learning_rate=2e-5,  # Back to standard learning rate\n",
    "    per_device_train_batch_size=16,  # Standard batch size\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=6,  # Fewer epochs to prevent overfitting\n",
    "    weight_decay=0.01,  # Standard weight decay\n",
    "    warmup_ratio=0.1,   # Standard warmup\n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",  # Still focus on macro-F1\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    dataloader_drop_last=False,\n",
    "    gradient_accumulation_steps=2,  # Standard accumulation\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Standard trainer - no over-engineering\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],  # Shorter patience\n",
    ")\n",
    "\n",
    "# Training\n",
    "logger.info(\"Starting conservative training approach...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./conservative_banglabert_hate_speech\")\n",
    "\n",
    "# Evaluation\n",
    "logger.info(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(\"Validation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        logger.info(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Detailed analysis\n",
    "val_predictions = trainer.predict(val_dataset)\n",
    "val_preds = np.argmax(val_predictions.predictions, axis=1)\n",
    "val_labels = val_dataset['label']\n",
    "\n",
    "logger.info(\"\\nValidation Classification Report:\")\n",
    "report = classification_report(val_labels, val_preds, \n",
    "                             target_names=list(hate_l2id.keys()), \n",
    "                             digits=4)\n",
    "logger.info(\"\\n\" + report)\n",
    "\n",
    "# Test predictions\n",
    "logger.info(\"Generating test predictions...\")\n",
    "test_prediction_dataset = test_dataset.remove_columns(['id'])\n",
    "test_predictions = trainer.predict(test_prediction_dataset)\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "# Save predictions\n",
    "output_file = \"./conservative_banglabert_hate_speech/subtask_1A.tsv\"\n",
    "os.makedirs(\"./conservative_banglabert_hate_speech\", exist_ok=True)\n",
    "\n",
    "with open(output_file, \"w\", encoding='utf-8') as writer:\n",
    "    writer.write(\"id\\tlabel\\tmodel\\n\")\n",
    "    for index, pred in enumerate(test_preds):\n",
    "        pred_label = id2hate[pred]\n",
    "        test_id = test_dataset['id'][index]\n",
    "        writer.write(f\"{test_id}\\t{pred_label}\\tconservative-banglabert\\n\")\n",
    "\n",
    "logger.info(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# Final scores\n",
    "final_f1_micro = eval_results.get('eval_f1_micro', 0)\n",
    "final_f1_macro = eval_results.get('eval_f1_macro', 0)\n",
    "\n",
    "logger.info(f\"\\n🎯 Final Results:\")\n",
    "logger.info(f\"F1-micro score: {final_f1_micro:.4f}\")\n",
    "logger.info(f\"F1-macro score: {final_f1_macro:.4f}\")\n",
    "\n",
    "if final_f1_macro >= 0.75:\n",
    "    logger.info(\"🎉 Excellent macro-F1 score achieved!\")\n",
    "elif final_f1_macro >= 0.70:\n",
    "    logger.info(\"✅ Good macro-F1 score achieved!\")\n",
    "else:\n",
    "    logger.info(\"📈 Need further tuning for better macro-F1\")\n",
    "\n",
    "logger.info(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
