{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4762ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.55.2\n",
      "PyTorch version: 2.7.1+cu126\n",
      "09/25/2025 04:06:39 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/25/2025 04:06:39 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/25/2025 04:06:39 - INFO - __main__ - Original class distribution: {0: 23373, 1: 676, 2: 122, 3: 4227, 4: 2331, 5: 8212}\n",
      "09/25/2025 04:06:39 - INFO - __main__ - Augmented label 1 (Religious Hate) from 676 to 1352\n",
      "09/25/2025 04:06:39 - INFO - __main__ - Augmented label 2 (Sexism) from 122 to 2000\n",
      "09/25/2025 04:06:39 - INFO - __main__ - Training data: 38941 -> 38122 samples\n",
      "09/25/2025 04:06:39 - INFO - __main__ - Final train label distribution:\n",
      "label\n",
      "0    20000\n",
      "1     1352\n",
      "2     2000\n",
      "3     4227\n",
      "4     2331\n",
      "5     8212\n",
      "Name: count, dtype: int64\n",
      "09/25/2025 04:06:39 - INFO - __main__ - Validation label distribution:\n",
      "label\n",
      "0    1451\n",
      "1      38\n",
      "2      11\n",
      "3     291\n",
      "4     157\n",
      "5     564\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38122/38122 [00:03<00:00, 10300.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2512/2512 [00:00<00:00, 7599.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10200/10200 [00:00<00:00, 10783.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 04:06:45 - INFO - __main__ - Focal loss weights: [1.0, 2.243964433670044, 1.9952622652053833, 1.5940403938293457, 1.9056639671325684, 1.306093692779541]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 04:06:47 - INFO - __main__ - Model loaded on cuda\n",
      "09/25/2025 04:06:47 - INFO - __main__ - Total parameters: 110,225,158\n",
      "09/25/2025 04:06:47 - INFO - __main__ - Trainable parameters: 110,225,158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 04:06:47 - INFO - __main__ - Starting optimized training for F1-micro 85%+...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Local\\Temp\\ipykernel_8096\\1288608052.py:345: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='9536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/9536 16:57 < 36:58, 2.95 it/s, Epoch 2/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Class 0 None</th>\n",
       "      <th>F1 Class 1 Religious Hate</th>\n",
       "      <th>F1 Class 2 Sexism</th>\n",
       "      <th>F1 Class 3 Political Hate</th>\n",
       "      <th>F1 Class 4 Profane</th>\n",
       "      <th>F1 Class 5 Abusive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.683400</td>\n",
       "      <td>1.261845</td>\n",
       "      <td>0.578025</td>\n",
       "      <td>0.129932</td>\n",
       "      <td>0.433836</td>\n",
       "      <td>0.732926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.555100</td>\n",
       "      <td>0.957939</td>\n",
       "      <td>0.653264</td>\n",
       "      <td>0.352760</td>\n",
       "      <td>0.633890</td>\n",
       "      <td>0.813472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387879</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.381877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.165400</td>\n",
       "      <td>0.721555</td>\n",
       "      <td>0.685111</td>\n",
       "      <td>0.522019</td>\n",
       "      <td>0.683783</td>\n",
       "      <td>0.806228</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.526502</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.476364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.887300</td>\n",
       "      <td>0.710939</td>\n",
       "      <td>0.621417</td>\n",
       "      <td>0.514150</td>\n",
       "      <td>0.640515</td>\n",
       "      <td>0.716542</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.530457</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.502256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.777100</td>\n",
       "      <td>0.671678</td>\n",
       "      <td>0.678344</td>\n",
       "      <td>0.531683</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.783916</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.715847</td>\n",
       "      <td>0.534586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.698700</td>\n",
       "      <td>0.653703</td>\n",
       "      <td>0.716162</td>\n",
       "      <td>0.518547</td>\n",
       "      <td>0.700533</td>\n",
       "      <td>0.823944</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525097</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.481720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.672638</td>\n",
       "      <td>0.680334</td>\n",
       "      <td>0.545590</td>\n",
       "      <td>0.690198</td>\n",
       "      <td>0.770904</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.573883</td>\n",
       "      <td>0.757835</td>\n",
       "      <td>0.551562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.646700</td>\n",
       "      <td>0.650036</td>\n",
       "      <td>0.679936</td>\n",
       "      <td>0.515436</td>\n",
       "      <td>0.689206</td>\n",
       "      <td>0.788348</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.559541</td>\n",
       "      <td>0.721763</td>\n",
       "      <td>0.522962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.643010</td>\n",
       "      <td>0.704220</td>\n",
       "      <td>0.514279</td>\n",
       "      <td>0.702283</td>\n",
       "      <td>0.809312</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574568</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.510848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.614400</td>\n",
       "      <td>0.622733</td>\n",
       "      <td>0.718949</td>\n",
       "      <td>0.561870</td>\n",
       "      <td>0.723173</td>\n",
       "      <td>0.813163</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.787172</td>\n",
       "      <td>0.576786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>0.645217</td>\n",
       "      <td>0.673965</td>\n",
       "      <td>0.563303</td>\n",
       "      <td>0.687724</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.544811</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.560811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.617104</td>\n",
       "      <td>0.674761</td>\n",
       "      <td>0.520488</td>\n",
       "      <td>0.685811</td>\n",
       "      <td>0.774517</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562577</td>\n",
       "      <td>0.746518</td>\n",
       "      <td>0.529514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.639534</td>\n",
       "      <td>0.671576</td>\n",
       "      <td>0.517565</td>\n",
       "      <td>0.684617</td>\n",
       "      <td>0.765053</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.557214</td>\n",
       "      <td>0.774929</td>\n",
       "      <td>0.546656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>0.654286</td>\n",
       "      <td>0.713376</td>\n",
       "      <td>0.524677</td>\n",
       "      <td>0.712405</td>\n",
       "      <td>0.810735</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585139</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.549020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.645724</td>\n",
       "      <td>0.709395</td>\n",
       "      <td>0.526576</td>\n",
       "      <td>0.713373</td>\n",
       "      <td>0.802608</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579104</td>\n",
       "      <td>0.761628</td>\n",
       "      <td>0.571674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 04:23:58 - INFO - __main__ - Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 04:24:09 - INFO - __main__ - Validation Results:\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_loss: 0.6227\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_micro: 0.7189\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_macro: 0.5619\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_weighted: 0.7232\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_class_0_None: 0.8132\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_class_1_Religious_Hate: 0.4203\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_class_2_Sexism: 0.1905\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_class_3_Political_Hate: 0.5833\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_class_4_Profane: 0.7872\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_f1_class_5_Abusive: 0.5768\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_runtime: 10.8312\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_samples_per_second: 231.9230\n",
      "09/25/2025 04:24:09 - INFO - __main__ - eval_steps_per_second: 7.2940\n",
      "09/25/2025 04:24:19 - INFO - __main__ - \n",
      "Validation Classification Report:\n",
      "09/25/2025 04:24:20 - INFO - __main__ - \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          None     0.8356    0.7919    0.8132      1451\n",
      "Religious Hate     0.2900    0.7632    0.4203        38\n",
      "        Sexism     0.2000    0.1818    0.1905        11\n",
      "Political Hate     0.5895    0.5773    0.5833       291\n",
      "       Profane     0.7258    0.8599    0.7872       157\n",
      "       Abusive     0.5809    0.5727    0.5768       564\n",
      "\n",
      "      accuracy                         0.7189      2512\n",
      "     macro avg     0.5370    0.6245    0.5619      2512\n",
      "  weighted avg     0.7320    0.7189    0.7232      2512\n",
      "\n",
      "09/25/2025 04:24:20 - INFO - __main__ - Generating test predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 04:24:51 - INFO - __main__ - Predictions saved to ./optimized_simple_banglabert/subtask_1A.tsv\n",
      "09/25/2025 04:24:51 - INFO - __main__ - \n",
      "ðŸŽ¯ Final Results:\n",
      "09/25/2025 04:24:51 - INFO - __main__ - F1-micro score: 0.7189\n",
      "09/25/2025 04:24:51 - INFO - __main__ - F1-macro score: 0.5619\n",
      "09/25/2025 04:24:51 - INFO - __main__ - ðŸ“ˆ Significant improvement achieved, consider ensemble\n",
      "09/25/2025 04:24:51 - INFO - __main__ - Training completed!\n"
     ]
    }
   ],
   "source": [
    "#33333333333333333\n",
    "# Enhanced Bangla Hate Speech Classification - Fixed and Optimized for F1-Micro 85%+\n",
    "# Key improvements: Better data balance, focal loss, simplified architecture\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim import AdamW\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Dataset paths\n",
    "train_file = 'merged_dataset.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1A_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1A_test.tsv'\n",
    "\n",
    "# Enhanced text preprocessing\n",
    "def clean_bangla_text(text):\n",
    "    \"\"\"Enhanced preprocessing for Bangla text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[à¥¤!?]{3,}', 'à¥¤à¥¤', text)\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Labels\n",
    "hate_l2id = {'None': 0, 'Religious Hate': 1, 'Sexism': 2, 'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\n",
    "id2hate = {v: k for k, v in hate_l2id.items()}\n",
    "num_labels = len(hate_l2id)\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_clean_dataset(file_path, is_test=False):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df['text'] = df['text'].apply(clean_bangla_text)\n",
    "    df = df[df['text'].str.len() > 0]\n",
    "    \n",
    "    if not is_test:\n",
    "        df['label'] = df['label'].map(hate_l2id)\n",
    "        if df['label'].isna().any():\n",
    "            logger.warning(f\"Unmapped labels found, filling with 0\")\n",
    "            df['label'] = df['label'].fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = load_and_clean_dataset(train_file)\n",
    "val_df = load_and_clean_dataset(validation_file)\n",
    "test_df = load_and_clean_dataset(test_file, is_test=True)\n",
    "\n",
    "# BALANCED data augmentation strategy\n",
    "def balanced_augmentation(df):\n",
    "    \"\"\"Smart augmentation targeting F1-micro improvement\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    class_counts = df['label'].value_counts().sort_index()\n",
    "    logger.info(f\"Original class distribution: {class_counts.to_dict()}\")\n",
    "    \n",
    "    # Calculate target sizes more strategically\n",
    "    max_count = class_counts.max()\n",
    "    \n",
    "    for label in df['label'].unique():\n",
    "        label_data = df[df['label'] == label].copy()\n",
    "        current_count = len(label_data)\n",
    "        \n",
    "        # Strategic augmentation based on class size\n",
    "        if label == 0:  # None class - slight reduction through undersampling\n",
    "            # Keep most but not all\n",
    "            target_count = min(current_count, 20000)\n",
    "            if target_count < current_count:\n",
    "                label_data = label_data.sample(n=target_count, random_state=42)\n",
    "        else:  # Minority classes - boost them\n",
    "            if current_count < 500:\n",
    "                target_count = 2000  # Boost very small classes significantly\n",
    "            elif current_count < 2000:\n",
    "                target_count = min(3000, current_count * 2)  # Moderate boost\n",
    "            else:\n",
    "                target_count = current_count  # Keep larger classes as is\n",
    "            \n",
    "            needed = max(0, target_count - current_count)\n",
    "            \n",
    "            if needed > 0:\n",
    "                for _ in range(needed):\n",
    "                    sample = label_data.sample(1).iloc[0]\n",
    "                    text = sample['text']\n",
    "                    \n",
    "                    # Simple but effective augmentation\n",
    "                    aug_type = random.choice(['duplicate', 'shuffle'])\n",
    "                    \n",
    "                    if aug_type == 'shuffle' and len(text.split()) > 4:\n",
    "                        words = text.split()\n",
    "                        # Shuffle only 2-3 words in the middle\n",
    "                        if len(words) >= 6:\n",
    "                            start = len(words) // 3\n",
    "                            end = min(start + 3, len(words) - 1)\n",
    "                            middle_words = words[start:end]\n",
    "                            random.shuffle(middle_words)\n",
    "                            words[start:end] = middle_words\n",
    "                            text = ' '.join(words)\n",
    "                    \n",
    "                    augmented_data.append({'text': text, 'label': label})\n",
    "                \n",
    "                logger.info(f\"Augmented label {label} ({id2hate[label]}) from {current_count} to {target_count}\")\n",
    "        \n",
    "        # Add original or sampled data\n",
    "        for _, row in label_data.iterrows():\n",
    "            augmented_data.append({'text': row['text'], 'label': row['label']})\n",
    "    \n",
    "    result_df = pd.DataFrame(augmented_data)\n",
    "    return result_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Apply balanced augmentation\n",
    "original_train_size = len(train_df)\n",
    "train_df = balanced_augmentation(train_df)\n",
    "logger.info(f\"Training data: {original_train_size} -> {len(train_df)} samples\")\n",
    "\n",
    "# Log final distributions\n",
    "logger.info(\"Final train label distribution:\\n%s\", train_df['label'].value_counts().sort_index())\n",
    "logger.info(\"Validation label distribution:\\n%s\", val_df['label'].value_counts().sort_index())\n",
    "\n",
    "# Convert to datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Model setup\n",
    "model_name = 'csebuetnlp/banglabert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "max_seq_length = 384  # Keep original length for better context\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Clean datasets\n",
    "train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names \n",
    "                                            if col not in ['input_ids', 'attention_mask', 'label']])\n",
    "val_dataset = val_dataset.remove_columns([col for col in val_dataset.column_names \n",
    "                                        if col not in ['input_ids', 'attention_mask', 'label']])\n",
    "test_columns_to_keep = ['input_ids', 'attention_mask', 'id']\n",
    "test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names \n",
    "                                          if col not in test_columns_to_keep])\n",
    "\n",
    "# Focal Loss implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Calculate balanced class weights\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "focal_weights = []\n",
    "\n",
    "for i, count in enumerate(class_counts):\n",
    "    if i == 0:  # None class\n",
    "        weight = 1.0\n",
    "    else:  # Other classes\n",
    "        weight = (class_counts[0] / count) ** 0.3  # Much gentler weighting\n",
    "    focal_weights.append(weight)\n",
    "\n",
    "focal_weights = torch.tensor(focal_weights, dtype=torch.float)\n",
    "logger.info(f\"Focal loss weights: {focal_weights.tolist()}\")\n",
    "\n",
    "# Simplified but effective model\n",
    "class SimplifiedOptimizedClassifier(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels, class_weights=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Base transformer - NO FREEZING\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        # Simple but effective head\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Simple 2-layer classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "        \n",
    "        # Store class weights as buffer (will be moved to device with model)\n",
    "        if class_weights is not None:\n",
    "            self.register_buffer('class_weights', class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        \n",
    "        # Use focal loss\n",
    "        self.focal_loss = FocalLoss(alpha=None, gamma=1.5)  # Will set alpha in forward\n",
    "        \n",
    "        # Initialize classifier weights\n",
    "        for module in self.classifier:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get transformer outputs\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use CLS token (first token) representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Use focal loss with proper device placement\n",
    "            self.focal_loss.alpha = self.class_weights\n",
    "            loss = self.focal_loss(logits, labels)\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss} if loss is not None else {'logits': logits}\n",
    "\n",
    "# Initialize model\n",
    "model = SimplifiedOptimizedClassifier(model_name, num_labels, class_weights=focal_weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "logger.info(f\"Model loaded on {device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Total parameters: {total_params:,}\")\n",
    "logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    f1_weighted = f1_score(labels, preds, average=\"weighted\")\n",
    "    f1_per_class = f1_score(labels, preds, average=None)\n",
    "    \n",
    "    result = {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "    }\n",
    "    \n",
    "    for i, f1 in enumerate(f1_per_class):\n",
    "        class_name = id2hate[i].replace(' ', '_')\n",
    "        result[f\"f1_class_{i}_{class_name}\"] = f1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Optimized training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./optimized_simple_banglabert\",\n",
    "    learning_rate=2e-5,  # Conservative learning rate\n",
    "    per_device_train_batch_size=16,  # Conservative batch size\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=8,  # More epochs\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",  # Target F1-micro\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    dataloader_drop_last=False,\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = 32\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"linear\",  # Simple linear scheduler\n",
    ")\n",
    "\n",
    "# Standard trainer (no custom optimizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "# Training\n",
    "logger.info(\"Starting optimized training for F1-micro 85%+...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./optimized_simple_banglabert\")\n",
    "\n",
    "# Evaluation\n",
    "logger.info(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(\"Validation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        logger.info(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Detailed analysis\n",
    "val_predictions = trainer.predict(val_dataset)\n",
    "val_preds = np.argmax(val_predictions.predictions, axis=1)\n",
    "val_labels = val_dataset['label']\n",
    "\n",
    "logger.info(\"\\nValidation Classification Report:\")\n",
    "report = classification_report(val_labels, val_preds, \n",
    "                             target_names=list(hate_l2id.keys()), \n",
    "                             digits=4)\n",
    "logger.info(\"\\n\" + report)\n",
    "\n",
    "# Test predictions\n",
    "logger.info(\"Generating test predictions...\")\n",
    "test_prediction_dataset = test_dataset.remove_columns(['id'])\n",
    "test_predictions = trainer.predict(test_prediction_dataset)\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "# Save predictions\n",
    "output_file = \"./optimized_simple_banglabert/subtask_1A.tsv\"\n",
    "os.makedirs(\"./optimized_simple_banglabert\", exist_ok=True)\n",
    "\n",
    "with open(output_file, \"w\", encoding='utf-8') as writer:\n",
    "    writer.write(\"id\\tlabel\\tmodel\\n\")\n",
    "    for index, pred in enumerate(test_preds):\n",
    "        pred_label = id2hate[pred]\n",
    "        test_id = test_dataset['id'][index]\n",
    "        writer.write(f\"{test_id}\\t{pred_label}\\toptimized-simple-banglabert\\n\")\n",
    "\n",
    "logger.info(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# Final scores\n",
    "final_f1_micro = eval_results.get('eval_f1_micro', 0)\n",
    "final_f1_macro = eval_results.get('eval_f1_macro', 0)\n",
    "\n",
    "logger.info(f\"\\nðŸŽ¯ Final Results:\")\n",
    "logger.info(f\"F1-micro score: {final_f1_micro:.4f}\")\n",
    "logger.info(f\"F1-macro score: {final_f1_macro:.4f}\")\n",
    "\n",
    "if final_f1_micro >= 0.85:\n",
    "    logger.info(\"ðŸŽ‰ TARGET ACHIEVED! F1-micro >= 85%!\")\n",
    "elif final_f1_micro >= 0.80:\n",
    "    logger.info(\"âœ… Very close! Try ensemble approach next\")\n",
    "else:\n",
    "    logger.info(\"ðŸ“ˆ Significant improvement achieved, consider ensemble\")\n",
    "\n",
    "logger.info(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
