{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25cf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.55.2\n",
      "PyTorch version: 2.7.1+cu126\n",
      "09/27/2025 04:36:10 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/27/2025 04:36:10 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/27/2025 04:36:10 - INFO - __main__ - Original class distribution: {0: 23373, 1: 676, 2: 122, 3: 4227, 4: 2331, 5: 8212}\n",
      "09/27/2025 08:34:52 - INFO - __main__ - Final train label distribution:\n",
      "label\n",
      "0    20000\n",
      "1     8000\n",
      "2     8000\n",
      "3     8000\n",
      "4     8000\n",
      "5     8212\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60212/60212 [00:04<00:00, 13943.15 examples/s]\n",
      "Map: 100%|██████████| 2512/2512 [00:00<00:00, 15822.17 examples/s]\n",
      "Map: 100%|██████████| 10200/10200 [00:00<00:00, 13342.21 examples/s]\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/27/2025 08:34:59 - INFO - __main__ - Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Local\\Temp\\ipykernel_5492\\2611596513.py:324: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLRDTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = LLRDTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='22584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6000/22584 44:42 < 2:03:35, 2.24 it/s, Epoch 3/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Class 0 None</th>\n",
       "      <th>F1 Class 1 Religious Hate</th>\n",
       "      <th>F1 Class 2 Sexism</th>\n",
       "      <th>F1 Class 3 Political Hate</th>\n",
       "      <th>F1 Class 4 Profane</th>\n",
       "      <th>F1 Class 5 Abusive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.772100</td>\n",
       "      <td>1.453282</td>\n",
       "      <td>0.280255</td>\n",
       "      <td>0.135678</td>\n",
       "      <td>0.315554</td>\n",
       "      <td>0.480325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.131469</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.066165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.666900</td>\n",
       "      <td>1.393439</td>\n",
       "      <td>0.333599</td>\n",
       "      <td>0.210729</td>\n",
       "      <td>0.380042</td>\n",
       "      <td>0.475266</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.239631</td>\n",
       "      <td>0.250608</td>\n",
       "      <td>0.275391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.455400</td>\n",
       "      <td>1.196298</td>\n",
       "      <td>0.390525</td>\n",
       "      <td>0.241435</td>\n",
       "      <td>0.416764</td>\n",
       "      <td>0.556273</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.297090</td>\n",
       "      <td>0.395570</td>\n",
       "      <td>0.160112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.241400</td>\n",
       "      <td>1.027297</td>\n",
       "      <td>0.536226</td>\n",
       "      <td>0.325929</td>\n",
       "      <td>0.547585</td>\n",
       "      <td>0.725369</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.228947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.137500</td>\n",
       "      <td>0.926620</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.397422</td>\n",
       "      <td>0.629285</td>\n",
       "      <td>0.789305</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.378495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.062100</td>\n",
       "      <td>0.890364</td>\n",
       "      <td>0.606688</td>\n",
       "      <td>0.407587</td>\n",
       "      <td>0.620941</td>\n",
       "      <td>0.761251</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.433862</td>\n",
       "      <td>0.595556</td>\n",
       "      <td>0.403670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.959100</td>\n",
       "      <td>0.855243</td>\n",
       "      <td>0.624204</td>\n",
       "      <td>0.423442</td>\n",
       "      <td>0.633918</td>\n",
       "      <td>0.768942</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.483912</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.409311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.901900</td>\n",
       "      <td>0.828881</td>\n",
       "      <td>0.641322</td>\n",
       "      <td>0.459079</td>\n",
       "      <td>0.647383</td>\n",
       "      <td>0.790663</td>\n",
       "      <td>0.303797</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.477064</td>\n",
       "      <td>0.718663</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.858400</td>\n",
       "      <td>0.815451</td>\n",
       "      <td>0.645701</td>\n",
       "      <td>0.482310</td>\n",
       "      <td>0.660109</td>\n",
       "      <td>0.771702</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.517766</td>\n",
       "      <td>0.690722</td>\n",
       "      <td>0.472761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.773500</td>\n",
       "      <td>0.808651</td>\n",
       "      <td>0.653264</td>\n",
       "      <td>0.479083</td>\n",
       "      <td>0.662437</td>\n",
       "      <td>0.784639</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.523754</td>\n",
       "      <td>0.673317</td>\n",
       "      <td>0.450505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.752600</td>\n",
       "      <td>0.820162</td>\n",
       "      <td>0.651274</td>\n",
       "      <td>0.475352</td>\n",
       "      <td>0.655792</td>\n",
       "      <td>0.790923</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>0.621145</td>\n",
       "      <td>0.416107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.749611</td>\n",
       "      <td>0.680732</td>\n",
       "      <td>0.505489</td>\n",
       "      <td>0.692229</td>\n",
       "      <td>0.786171</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.550079</td>\n",
       "      <td>0.758017</td>\n",
       "      <td>0.542880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.749700</td>\n",
       "      <td>0.771572</td>\n",
       "      <td>0.646497</td>\n",
       "      <td>0.504588</td>\n",
       "      <td>0.662998</td>\n",
       "      <td>0.750801</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.538365</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>0.515713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.756810</td>\n",
       "      <td>0.655653</td>\n",
       "      <td>0.499564</td>\n",
       "      <td>0.670368</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.698980</td>\n",
       "      <td>0.516387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.783112</td>\n",
       "      <td>0.628981</td>\n",
       "      <td>0.495069</td>\n",
       "      <td>0.646542</td>\n",
       "      <td>0.733114</td>\n",
       "      <td>0.361446</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.530466</td>\n",
       "      <td>0.691542</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.738439</td>\n",
       "      <td>0.662022</td>\n",
       "      <td>0.528461</td>\n",
       "      <td>0.675870</td>\n",
       "      <td>0.761265</td>\n",
       "      <td>0.373626</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.559367</td>\n",
       "      <td>0.721485</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.645500</td>\n",
       "      <td>0.735737</td>\n",
       "      <td>0.681927</td>\n",
       "      <td>0.531023</td>\n",
       "      <td>0.693905</td>\n",
       "      <td>0.790236</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.537037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.670600</td>\n",
       "      <td>0.738269</td>\n",
       "      <td>0.680732</td>\n",
       "      <td>0.523432</td>\n",
       "      <td>0.689447</td>\n",
       "      <td>0.795489</td>\n",
       "      <td>0.379747</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.548544</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.516811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.788259</td>\n",
       "      <td>0.617038</td>\n",
       "      <td>0.509964</td>\n",
       "      <td>0.636268</td>\n",
       "      <td>0.704467</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.530425</td>\n",
       "      <td>0.723861</td>\n",
       "      <td>0.516178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.616400</td>\n",
       "      <td>0.751135</td>\n",
       "      <td>0.655653</td>\n",
       "      <td>0.532132</td>\n",
       "      <td>0.672599</td>\n",
       "      <td>0.755805</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.533040</td>\n",
       "      <td>0.763848</td>\n",
       "      <td>0.534247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.730358</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>0.571270</td>\n",
       "      <td>0.704340</td>\n",
       "      <td>0.797430</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.558313</td>\n",
       "      <td>0.743169</td>\n",
       "      <td>0.553459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>0.738893</td>\n",
       "      <td>0.654459</td>\n",
       "      <td>0.539698</td>\n",
       "      <td>0.669525</td>\n",
       "      <td>0.744016</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.559278</td>\n",
       "      <td>0.738544</td>\n",
       "      <td>0.540582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.583400</td>\n",
       "      <td>0.734540</td>\n",
       "      <td>0.673169</td>\n",
       "      <td>0.542607</td>\n",
       "      <td>0.687673</td>\n",
       "      <td>0.772781</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.558313</td>\n",
       "      <td>0.763533</td>\n",
       "      <td>0.542688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.563700</td>\n",
       "      <td>0.698391</td>\n",
       "      <td>0.708997</td>\n",
       "      <td>0.543917</td>\n",
       "      <td>0.715486</td>\n",
       "      <td>0.807522</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.575380</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.568458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.574700</td>\n",
       "      <td>0.727391</td>\n",
       "      <td>0.689490</td>\n",
       "      <td>0.547827</td>\n",
       "      <td>0.699217</td>\n",
       "      <td>0.796380</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.554885</td>\n",
       "      <td>0.735135</td>\n",
       "      <td>0.540037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.566000</td>\n",
       "      <td>0.701831</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.540747</td>\n",
       "      <td>0.699475</td>\n",
       "      <td>0.781457</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.569482</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.568520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.582400</td>\n",
       "      <td>0.702421</td>\n",
       "      <td>0.692675</td>\n",
       "      <td>0.548889</td>\n",
       "      <td>0.702748</td>\n",
       "      <td>0.790255</td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.560203</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.561373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.571500</td>\n",
       "      <td>0.698666</td>\n",
       "      <td>0.697452</td>\n",
       "      <td>0.570264</td>\n",
       "      <td>0.706854</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.426966</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.571816</td>\n",
       "      <td>0.764873</td>\n",
       "      <td>0.564544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.523700</td>\n",
       "      <td>0.731785</td>\n",
       "      <td>0.687102</td>\n",
       "      <td>0.552191</td>\n",
       "      <td>0.698488</td>\n",
       "      <td>0.786697</td>\n",
       "      <td>0.395062</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>0.553304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.704965</td>\n",
       "      <td>0.681131</td>\n",
       "      <td>0.544109</td>\n",
       "      <td>0.693733</td>\n",
       "      <td>0.773940</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.566580</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.559496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/27/2025 09:19:47 - INFO - __main__ - Evaluating with snapshot ensemble...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/27/2025 09:20:00 - INFO - __main__ -                 precision    recall  f1-score   support\n",
      "\n",
      "          None     0.8684    0.7547    0.8075      1451\n",
      "Religious Hate     0.4667    0.3684    0.4118        38\n",
      "        Sexism     0.2500    0.0909    0.1333        11\n",
      "Political Hate     0.4815    0.7148    0.5754       291\n",
      "       Profane     0.6923    0.8599    0.7670       157\n",
      "       Abusive     0.5559    0.5816    0.5685       564\n",
      "\n",
      "      accuracy                         0.7090      2512\n",
      "     macro avg     0.5525    0.5617    0.5439      2512\n",
      "  weighted avg     0.7336    0.7090    0.7155      2512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Bangla Hate Speech Classification - v2\n",
    "Additions: \n",
    " - Back-translation augmentation (Bangla <-> English)\n",
    " - Logit-Adjusted Loss (in addition to CB-Focal)\n",
    " - Snapshot Ensembling for inference\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "train_file = 'merged_dataset.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1A_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1A_test.tsv'\n",
    "\n",
    "def clean_bangla_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[।!?]{3,}', '।।', text)\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "hate_l2id = {'None': 0, 'Religious Hate': 1, 'Sexism': 2,\n",
    "             'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\n",
    "id2hate = {v: k for k, v in hate_l2id.items()}\n",
    "num_labels = len(hate_l2id)\n",
    "\n",
    "def load_and_clean_dataset(file_path, is_test=False):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df['text'] = df['text'].apply(clean_bangla_text)\n",
    "    df = df[df['text'].str.len() > 0]\n",
    "    if not is_test:\n",
    "        df['label'] = df['label'].map(hate_l2id)\n",
    "        if df['label'].isna().any():\n",
    "            logger.warning(\"Unmapped labels found, filling with 0\")\n",
    "            df['label'] = df['label'].fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = load_and_clean_dataset(train_file)\n",
    "val_df = load_and_clean_dataset(validation_file)\n",
    "test_df = load_and_clean_dataset(test_file, is_test=True)\n",
    "\n",
    "try:\n",
    "    from googletrans import Translator\n",
    "    translator = Translator()\n",
    "    def back_translate(text, src='bn', pivot='en'):\n",
    "        try:\n",
    "            en = translator.translate(text, src=src, dest=pivot).text\n",
    "            bn = translator.translate(en, src=pivot, dest=src).text\n",
    "            return bn\n",
    "        except:\n",
    "            return text\n",
    "except ImportError:\n",
    "    logger.warning(\"googletrans not installed, back-translation disabled\")\n",
    "    def back_translate(text, src='bn', pivot='en'): return text\n",
    "\n",
    "def balanced_augmentation_strong(df: pd.DataFrame, cap_none=20000, target_per_class=6000):\n",
    "    aug = []\n",
    "    class_counts = df['label'].value_counts().sort_index()\n",
    "    logger.info(f\"Original class distribution: {class_counts.to_dict()}\")\n",
    "    for label in sorted(df['label'].unique()):\n",
    "        cdf = df[df['label'] == label].copy()\n",
    "        if label == 0:\n",
    "            keep = min(len(cdf), cap_none)\n",
    "            cdf = cdf.sample(n=keep, random_state=42) if len(cdf) > keep else cdf\n",
    "            aug.extend(cdf.to_dict(orient='records'))\n",
    "        else:\n",
    "            need = max(0, target_per_class - len(cdf))\n",
    "            if need > 0:\n",
    "                base_texts = cdf['text'].tolist()\n",
    "                for _ in range(need):\n",
    "                    t = random.choice(base_texts)\n",
    "                    if label in [1, 2]: \n",
    "                        aug.append({'text': back_translate(t), 'label': label})\n",
    "                    else:\n",
    "                        aug.append({'text': t, 'label': label})\n",
    "            aug.extend(cdf.to_dict(orient='records'))\n",
    "    res = pd.DataFrame(aug)\n",
    "    return res.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df = balanced_augmentation_strong(train_df, cap_none=20000, target_per_class=8000)\n",
    "logger.info(\"Final train label distribution:\\n%s\", train_df['label'].value_counts().sort_index())\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "model_name = 'csebuetnlp/banglabert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "max_seq_length = 256\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([c for c in train_dataset.column_names if c not in ['input_ids','attention_mask','label']])\n",
    "val_dataset   = val_dataset.remove_columns([c for c in val_dataset.column_names if c not in ['input_ids','attention_mask','label']])\n",
    "test_dataset  = test_dataset.remove_columns([c for c in test_dataset.column_names if c in ['text']])\n",
    "\n",
    "class CBFocalLoss(nn.Module):\n",
    "    def __init__(self,class_counts:torch.Tensor,beta=0.9999,gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.register_buffer('alpha', self._compute_alpha(class_counts,beta))\n",
    "    @staticmethod\n",
    "    def _compute_alpha(cc:torch.Tensor,beta:float):\n",
    "        effective = 1.0 - torch.pow(torch.tensor(beta,dtype=torch.float,device=cc.device), cc.float())\n",
    "        weights = (1.0-beta)/(effective+1e-12)\n",
    "        return (weights/weights.mean()).float()\n",
    "    def forward(self,logits,targets):\n",
    "        log_probs = F.log_softmax(logits,dim=-1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        onehot = F.one_hot(targets,num_classes=logits.size(-1)).float()\n",
    "        focal = torch.pow((1.0-(probs*onehot).sum(dim=-1)), self.gamma)\n",
    "        alpha_t = (self.alpha[targets]).to(logits.dtype)\n",
    "        ce = -(log_probs*onehot).sum(dim=-1)\n",
    "        return (alpha_t*focal*ce).mean()\n",
    "\n",
    "class LogitAdjustedLoss(nn.Module):\n",
    "    \"\"\"Menon et al. 2020: logit-adjusted cross entropy for long-tail\"\"\"\n",
    "    def __init__(self, class_counts: torch.Tensor, tau: float = 1.0):\n",
    "        super().__init__()\n",
    "        freqs = class_counts / class_counts.sum()\n",
    "        self.register_buffer('bias', tau*torch.log(freqs+1e-12))\n",
    "    def forward(self, logits, targets):\n",
    "        adj_logits = logits + self.bias\n",
    "        return F.cross_entropy(adj_logits, targets)\n",
    "\n",
    "def kl_divergence_with_logits(p_logits,q_logits):\n",
    "    p = F.log_softmax(p_logits,dim=-1)\n",
    "    q = F.log_softmax(q_logits,dim=-1)\n",
    "    return (F.kl_div(p,q.exp(),reduction='batchmean')\n",
    "           +F.kl_div(q,p.exp(),reduction='batchmean'))/2.0\n",
    "\n",
    "class ResearchOptimizedClassifier(nn.Module):\n",
    "    def __init__(self, base_model_name,num_labels,class_counts,\n",
    "                 rdrop_alpha=3.0,multi_sample=5,dropout_p=0.2,use_logit_adjusted=False):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        self.base_model.gradient_checkpointing_enable()\n",
    "        self.num_labels = num_labels\n",
    "        self.rdrop_alpha = rdrop_alpha\n",
    "        self.multi_sample = multi_sample\n",
    "        self.use_logit_adjusted = use_logit_adjusted\n",
    "        hidden = self.base_model.config.hidden_size\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(dropout_p) for _ in range(multi_sample)])\n",
    "        self.head = nn.Linear(hidden,num_labels)\n",
    "        nn.init.xavier_uniform_(self.head.weight); nn.init.zeros_(self.head.bias)\n",
    "        self.cb_focal = CBFocalLoss(class_counts)\n",
    "        self.logit_adj = LogitAdjustedLoss(class_counts)\n",
    "\n",
    "    def _mean_pool(self,last_hidden_state,mask):\n",
    "        mask = mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "        summed = (last_hidden_state*mask).sum(dim=1)\n",
    "        counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        return summed/counts\n",
    "\n",
    "    def _logits_once(self,input_ids,mask):\n",
    "        outputs = self.base_model(input_ids=input_ids,attention_mask=mask)\n",
    "        pooled = self._mean_pool(outputs.last_hidden_state,mask)\n",
    "        logits = 0\n",
    "        for d in self.dropout_layers:\n",
    "            logits += self.head(d(pooled))\n",
    "        return logits/self.multi_sample\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,labels=None):\n",
    "        if labels is None:\n",
    "            return {'logits': self._logits_once(input_ids,attention_mask)}\n",
    "        logits1 = self._logits_once(input_ids,attention_mask)\n",
    "        logits2 = self._logits_once(input_ids,attention_mask)\n",
    "        if self.use_logit_adjusted:\n",
    "            ce1 = self.logit_adj(logits1,labels); ce2 = self.logit_adj(logits2,labels)\n",
    "        else:\n",
    "            ce1 = self.cb_focal(logits1,labels);   ce2 = self.cb_focal(logits2,labels)\n",
    "        kl = kl_divergence_with_logits(logits1,logits2)\n",
    "        loss = (ce1+ce2)/2.0 + self.rdrop_alpha*kl\n",
    "        return {'logits':(logits1+logits2)/2.0,'loss':loss}\n",
    "\n",
    "counts = torch.tensor(train_df['label'].value_counts().sort_index().values,dtype=torch.float)\n",
    "model = ResearchOptimizedClassifier(model_name,num_labels,counts,use_logit_adjusted=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds,axis=1)\n",
    "    f1_micro = f1_score(labels,preds,average=\"micro\")\n",
    "    f1_macro = f1_score(labels,preds,average=\"macro\")\n",
    "    f1_weighted = f1_score(labels,preds,average=\"weighted\")\n",
    "    result = {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "    }\n",
    "    for i,f in enumerate(f1_score(labels,preds,average=None)):\n",
    "        result[f\"f1_class_{i}_{id2hate[i].replace(' ','_')}\"] = f\n",
    "    return result\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./optimized_simple_banglabert_v2\",\n",
    "    learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=32,\n",
    "    num_train_epochs=12, weight_decay=0.01,warmup_ratio=0.1,\n",
    "    logging_steps=100,eval_steps=200,save_steps=200,save_total_limit=5,\n",
    "    eval_strategy=\"steps\",save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True, metric_for_best_model=\"f1_micro\",greater_is_better=True,\n",
    "    gradient_accumulation_steps=2,fp16=True,dataloader_num_workers=2, max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"cosine\",report_to=None\n",
    ")\n",
    "\n",
    "class LLRDTrainer(Trainer):\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is None:\n",
    "            base_lr = 1e-5; head_lr = 2e-4; weight_decay = self.args.weight_decay; layer_decay=0.9\n",
    "            no_decay = [\"bias\",\"LayerNorm.weight\"]\n",
    "            base_model = self.model.base_model\n",
    "            param_groups=[]; layers=list(base_model.encoder.layer); n=len(layers)\n",
    "            for i,layer in enumerate(layers):\n",
    "                depth=n-i-1; lr_i=base_lr*(layer_decay**depth)\n",
    "                param_groups.append({\"params\":[p for n_,p in layer.named_parameters() if not any(nd in n_ for nd in no_decay)],\n",
    "                                     \"weight_decay\":weight_decay,\"lr\":lr_i})\n",
    "                param_groups.append({\"params\":[p for n_,p in layer.named_parameters() if any(nd in n_ for nd in no_decay)],\n",
    "                                     \"weight_decay\":0.0,\"lr\":lr_i})\n",
    "            head_params=list(self.model.head.named_parameters())\n",
    "            param_groups.append({\"params\":[p for n_,p in head_params if not any(nd in n_ for nd in no_decay)],\n",
    "                                \"weight_decay\":weight_decay,\"lr\":head_lr})\n",
    "            param_groups.append({\"params\":[p for n_,p in head_params if any(nd in n_ for nd in no_decay)],\n",
    "                                \"weight_decay\":0.0,\"lr\":head_lr})\n",
    "            self.optimizer=torch.optim.AdamW(param_groups,betas=(0.9,0.999),eps=1e-8)\n",
    "        return self.optimizer\n",
    "    def create_scheduler(self, num_training_steps,optimizer=None):\n",
    "        if self.lr_scheduler is None:\n",
    "            self.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer=self.optimizer,\n",
    "                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n",
    "                num_training_steps=num_training_steps)\n",
    "        return self.lr_scheduler\n",
    "\n",
    "trainer = LLRDTrainer(\n",
    "    model=model,args=training_args,\n",
    "    train_dataset=train_dataset,eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=6)],\n",
    ")\n",
    "\n",
    "logger.info(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "os.makedirs(\"./optimized_simple_banglabert_v2/snapshots\",exist_ok=True)\n",
    "trainer.save_model(\"./optimized_simple_banglabert_v2/snapshots/final\")\n",
    "tokenizer.save_pretrained(\"./optimized_simple_banglabert_v2\")\n",
    "\n",
    "logger.info(\"Evaluating with snapshot ensemble...\")\n",
    "preds_accum=None; n_models=0\n",
    "for ckpt in os.listdir(\"./optimized_simple_banglabert_v2/snapshots\"):\n",
    "    try:\n",
    "        m = ResearchOptimizedClassifier(model_name,num_labels,counts,use_logit_adjusted=True)\n",
    "        m.load_state_dict(torch.load(f\"./optimized_simple_banglabert_v2/snapshots/{ckpt}/pytorch_model.bin\"))\n",
    "        m.to(device); m.eval()\n",
    "        outputs=trainer.predict(val_dataset).predictions\n",
    "        if preds_accum is None: preds_accum=outputs\n",
    "        else: preds_accum+=outputs\n",
    "        n_models+=1\n",
    "    except: continue\n",
    "if n_models>0:\n",
    "    preds_accum/=n_models\n",
    "    preds=np.argmax(preds_accum,axis=1)\n",
    "    f1_micro=f1_score(val_dataset['label'],preds,average=\"micro\")\n",
    "    f1_macro=f1_score(val_dataset['label'],preds,average=\"macro\")\n",
    "    logger.info(f\"Snapshot ensemble F1-micro={f1_micro:.4f}, F1-macro={f1_macro:.4f}\")\n",
    "\n",
    "val_predictions = trainer.predict(val_dataset)\n",
    "val_preds = np.argmax(val_predictions.predictions, axis=1)\n",
    "logger.info(classification_report(val_dataset['label'],val_preds,target_names=list(hate_l2id.keys()),digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b75115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Rebuilding model architecture...\n",
      "Loading safetensors weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Local\\Temp\\ipykernel_5492\\184779061.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  inference_trainer = Trainer(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved to ./optimized_simple_banglabert_v2/subtask_1A.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, Trainer, default_data_collator\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# ==========================\n",
    "# Paths\n",
    "# ==========================\n",
    "model_name = \"csebuetnlp/banglabert\"   # same as training\n",
    "model_dir = \"./optimized_simple_banglabert_v2/snapshots/final\"\n",
    "tokenizer_dir = \"./optimized_simple_banglabert_v2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================\n",
    "# Reload tokenizer & model\n",
    "# ==========================\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "print(\"Rebuilding model architecture...\")\n",
    "# make sure num_labels & counts are defined same as training\n",
    "model = ResearchOptimizedClassifier(model_name, num_labels, counts, use_logit_adjusted=True)\n",
    "\n",
    "print(\"Loading safetensors weights...\")\n",
    "state_dict = load_file(f\"{model_dir}/model.safetensors\")\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ==========================\n",
    "# Prepare test dataset\n",
    "# (assuming you already built test_dataset in training script)\n",
    "# ==========================\n",
    "test_prediction_dataset = test_dataset.remove_columns(['id'])\n",
    "\n",
    "# Use Trainer only for prediction loop\n",
    "inference_trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "print(\"Running inference on test set...\")\n",
    "test_predictions = inference_trainer.predict(test_prediction_dataset)\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "# Map back to labels\n",
    "test_labels = [id2hate[p] for p in test_preds]\n",
    "\n",
    "# ==========================\n",
    "# Save predictions\n",
    "# ==========================\n",
    "output_file = \"./optimized_simple_banglabert_v2/subtask_1A.tsv\"\n",
    "os.makedirs(\"./optimized_simple_banglabert_v2\", exist_ok=True)\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "    writer.write(\"id\\tlabel\\tmodel\\n\")\n",
    "    for index, pred_label in enumerate(test_labels):\n",
    "        test_id = test_dataset['id'][index]\n",
    "        writer.write(f\"{test_id}\\t{pred_label}\\toptimized-simple-banglabert-v2\\n\")\n",
    "\n",
    "print(f\"✅ Predictions saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
