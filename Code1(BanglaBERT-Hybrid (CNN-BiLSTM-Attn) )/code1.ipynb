{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.55.2\n",
      "PyTorch version: 2.7.1+cu126\n",
      "09/25/2025 19:10:00 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/25/2025 19:10:00 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/25/2025 19:10:00 - INFO - __main__ - Augmented label 2 from 122 to 500 samples\n",
      "09/25/2025 19:10:00 - INFO - __main__ - Training data augmented from 38941 to 39319 samples\n",
      "09/25/2025 19:10:00 - INFO - __main__ - Train label distribution:\n",
      "label\n",
      "0    23373\n",
      "1      676\n",
      "2      500\n",
      "3     4227\n",
      "4     2331\n",
      "5     8212\n",
      "Name: count, dtype: int64\n",
      "09/25/2025 19:10:00 - INFO - __main__ - Validation label distribution:\n",
      "label\n",
      "0    1451\n",
      "1      38\n",
      "2      11\n",
      "3     291\n",
      "4     157\n",
      "5     564\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 39319/39319 [00:03<00:00, 10166.95 examples/s]\n",
      "Map: 100%|██████████| 2512/2512 [00:00<00:00, 10600.33 examples/s]\n",
      "Map: 100%|██████████| 10200/10200 [00:00<00:00, 10893.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 19:10:08 - INFO - __main__ - Model loaded on cuda\n",
      "09/25/2025 19:10:08 - INFO - __main__ - Model parameters: 122,802,246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 19:10:08 - INFO - __main__ - Starting training with enhanced model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Local\\Temp\\ipykernel_8784\\1068344296.py:395: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdvancedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = AdvancedTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6400' max='9832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6400/9832 52:23 < 28:06, 2.04 it/s, Epoch 5/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "      <th>F1 Class 2</th>\n",
       "      <th>F1 Class 3</th>\n",
       "      <th>F1 Class 4</th>\n",
       "      <th>F1 Class 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.484700</td>\n",
       "      <td>0.320378</td>\n",
       "      <td>0.087182</td>\n",
       "      <td>0.060986</td>\n",
       "      <td>0.052527</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185965</td>\n",
       "      <td>0.117035</td>\n",
       "      <td>0.035889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.321891</td>\n",
       "      <td>0.158439</td>\n",
       "      <td>0.103186</td>\n",
       "      <td>0.172182</td>\n",
       "      <td>0.218356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.133519</td>\n",
       "      <td>0.062112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.324055</td>\n",
       "      <td>0.239252</td>\n",
       "      <td>0.156844</td>\n",
       "      <td>0.271378</td>\n",
       "      <td>0.328628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273292</td>\n",
       "      <td>0.162025</td>\n",
       "      <td>0.177122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.264280</td>\n",
       "      <td>0.431927</td>\n",
       "      <td>0.242246</td>\n",
       "      <td>0.468918</td>\n",
       "      <td>0.632241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360622</td>\n",
       "      <td>0.255983</td>\n",
       "      <td>0.204629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.224427</td>\n",
       "      <td>0.527468</td>\n",
       "      <td>0.264265</td>\n",
       "      <td>0.534229</td>\n",
       "      <td>0.761285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410738</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.129985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.184694</td>\n",
       "      <td>0.582803</td>\n",
       "      <td>0.304044</td>\n",
       "      <td>0.572733</td>\n",
       "      <td>0.784343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.404321</td>\n",
       "      <td>0.191155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.165686</td>\n",
       "      <td>0.608280</td>\n",
       "      <td>0.354097</td>\n",
       "      <td>0.619719</td>\n",
       "      <td>0.777437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458735</td>\n",
       "      <td>0.505855</td>\n",
       "      <td>0.382557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.135221</td>\n",
       "      <td>0.650080</td>\n",
       "      <td>0.377764</td>\n",
       "      <td>0.645507</td>\n",
       "      <td>0.804965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.388222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.142462</td>\n",
       "      <td>0.614650</td>\n",
       "      <td>0.371054</td>\n",
       "      <td>0.628060</td>\n",
       "      <td>0.766326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481203</td>\n",
       "      <td>0.556075</td>\n",
       "      <td>0.422719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.117082</td>\n",
       "      <td>0.645303</td>\n",
       "      <td>0.379782</td>\n",
       "      <td>0.641357</td>\n",
       "      <td>0.792600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501594</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.394336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.114875</td>\n",
       "      <td>0.651274</td>\n",
       "      <td>0.398475</td>\n",
       "      <td>0.653420</td>\n",
       "      <td>0.786059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.440580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.109989</td>\n",
       "      <td>0.656051</td>\n",
       "      <td>0.402107</td>\n",
       "      <td>0.658106</td>\n",
       "      <td>0.780240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516432</td>\n",
       "      <td>0.635514</td>\n",
       "      <td>0.480458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.103012</td>\n",
       "      <td>0.676354</td>\n",
       "      <td>0.421409</td>\n",
       "      <td>0.676688</td>\n",
       "      <td>0.790025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541899</td>\n",
       "      <td>0.685552</td>\n",
       "      <td>0.510975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.100377</td>\n",
       "      <td>0.676752</td>\n",
       "      <td>0.424918</td>\n",
       "      <td>0.679336</td>\n",
       "      <td>0.787857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540107</td>\n",
       "      <td>0.694864</td>\n",
       "      <td>0.526678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.132400</td>\n",
       "      <td>0.096079</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.438846</td>\n",
       "      <td>0.686407</td>\n",
       "      <td>0.795224</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558865</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.523478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.095285</td>\n",
       "      <td>0.679140</td>\n",
       "      <td>0.421612</td>\n",
       "      <td>0.674733</td>\n",
       "      <td>0.788615</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.511364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.120900</td>\n",
       "      <td>0.093162</td>\n",
       "      <td>0.697054</td>\n",
       "      <td>0.442042</td>\n",
       "      <td>0.691935</td>\n",
       "      <td>0.813477</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556474</td>\n",
       "      <td>0.689459</td>\n",
       "      <td>0.503953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.092824</td>\n",
       "      <td>0.688694</td>\n",
       "      <td>0.444553</td>\n",
       "      <td>0.686995</td>\n",
       "      <td>0.795623</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562108</td>\n",
       "      <td>0.700637</td>\n",
       "      <td>0.521994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.089480</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>0.459101</td>\n",
       "      <td>0.691399</td>\n",
       "      <td>0.799025</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558989</td>\n",
       "      <td>0.700906</td>\n",
       "      <td>0.529020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.091032</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.450541</td>\n",
       "      <td>0.686569</td>\n",
       "      <td>0.793412</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.557545</td>\n",
       "      <td>0.691358</td>\n",
       "      <td>0.527596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.087810</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.454394</td>\n",
       "      <td>0.686698</td>\n",
       "      <td>0.792850</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558405</td>\n",
       "      <td>0.673139</td>\n",
       "      <td>0.531760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.090093</td>\n",
       "      <td>0.701035</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.694806</td>\n",
       "      <td>0.808467</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566906</td>\n",
       "      <td>0.689855</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.111400</td>\n",
       "      <td>0.094707</td>\n",
       "      <td>0.677946</td>\n",
       "      <td>0.458522</td>\n",
       "      <td>0.678744</td>\n",
       "      <td>0.798439</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532584</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.489883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.087989</td>\n",
       "      <td>0.696258</td>\n",
       "      <td>0.499662</td>\n",
       "      <td>0.693342</td>\n",
       "      <td>0.808200</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.558747</td>\n",
       "      <td>0.696165</td>\n",
       "      <td>0.505155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.088767</td>\n",
       "      <td>0.697850</td>\n",
       "      <td>0.498437</td>\n",
       "      <td>0.698058</td>\n",
       "      <td>0.806782</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.563265</td>\n",
       "      <td>0.707865</td>\n",
       "      <td>0.527859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>0.698248</td>\n",
       "      <td>0.516449</td>\n",
       "      <td>0.696946</td>\n",
       "      <td>0.809474</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.564767</td>\n",
       "      <td>0.706587</td>\n",
       "      <td>0.509927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>0.708997</td>\n",
       "      <td>0.515812</td>\n",
       "      <td>0.703348</td>\n",
       "      <td>0.814538</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.573964</td>\n",
       "      <td>0.711765</td>\n",
       "      <td>0.523858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.690685</td>\n",
       "      <td>0.521741</td>\n",
       "      <td>0.692694</td>\n",
       "      <td>0.799860</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.554960</td>\n",
       "      <td>0.656151</td>\n",
       "      <td>0.531746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.086007</td>\n",
       "      <td>0.695860</td>\n",
       "      <td>0.540941</td>\n",
       "      <td>0.696285</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.559110</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.523327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.097200</td>\n",
       "      <td>0.084269</td>\n",
       "      <td>0.693471</td>\n",
       "      <td>0.519822</td>\n",
       "      <td>0.691724</td>\n",
       "      <td>0.808116</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.560106</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.494670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.083780</td>\n",
       "      <td>0.701831</td>\n",
       "      <td>0.512619</td>\n",
       "      <td>0.698105</td>\n",
       "      <td>0.811673</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.573913</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.510903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.083102</td>\n",
       "      <td>0.708599</td>\n",
       "      <td>0.528972</td>\n",
       "      <td>0.707186</td>\n",
       "      <td>0.811614</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.539370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 20:02:44 - INFO - __main__ - Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 20:02:57 - INFO - __main__ - Validation Results:\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_loss: 0.0846\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_micro: 0.7090\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_macro: 0.5158\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_weighted: 0.7033\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_class_0: 0.8145\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_class_1: 0.2041\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_class_2: 0.2667\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_class_3: 0.5740\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_class_4: 0.7118\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_f1_class_5: 0.5239\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_runtime: 12.5594\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_samples_per_second: 200.0090\n",
      "09/25/2025 20:02:57 - INFO - __main__ - eval_steps_per_second: 6.2900\n",
      "09/25/2025 20:03:09 - INFO - __main__ - \n",
      "Validation Classification Report:\n",
      "09/25/2025 20:03:09 - INFO - __main__ - \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          None     0.8031    0.8263    0.8145      1451\n",
      "Religious Hate     0.4545    0.1316    0.2041        38\n",
      "        Sexism     0.2105    0.3636    0.2667        11\n",
      "Political Hate     0.5039    0.6667    0.5740       291\n",
      "       Profane     0.6612    0.7707    0.7118       157\n",
      "       Abusive     0.6128    0.4574    0.5239       564\n",
      "\n",
      "      accuracy                         0.7090      2512\n",
      "     macro avg     0.5410    0.5361    0.5158      2512\n",
      "  weighted avg     0.7090    0.7090    0.7033      2512\n",
      "\n",
      "09/25/2025 20:03:09 - INFO - __main__ - Generating test predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/25/2025 20:03:48 - INFO - __main__ - Predictions saved to ./advanced_banglabert_hate_speech/subtask_1A.tsv\n",
      "09/25/2025 20:03:48 - INFO - __main__ - Training completed successfully!\n",
      "09/25/2025 20:03:48 - INFO - __main__ - \n",
      "Final F1-micro score: 0.7090\n",
      "09/25/2025 20:03:48 - INFO - __main__ - Target not reached. Consider ensemble methods or further hyperparameter tuning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "g\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "train_file = 'merged_dataset.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1A_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1A_test.tsv'\n",
    "\n",
    "for file in [train_file, validation_file, test_file]:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"File {file} not found\")\n",
    "\n",
    "\n",
    "def clean_bangla_text(text):\n",
    "    \"\"\"Enhanced preprocessing for Bangla text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    " \n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    text = re.sub(r'[।!?]{2,}', '।', text)\n",
    "    \n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "hate_l2id = {'None': 0, 'Religious Hate': 1, 'Sexism': 2, 'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\n",
    "id2hate = {v: k for k, v in hate_l2id.items()}\n",
    "num_labels = len(hate_l2id)\n",
    "\n",
    "def load_and_clean_dataset(file_path, is_test=False):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df['text'] = df['text'].apply(clean_bangla_text)\n",
    "\n",
    "    df = df[df['text'].str.len() > 0]\n",
    "    \n",
    "    if not is_test:\n",
    "        df['label'] = df['label'].map(hate_l2id)\n",
    "        if df['label'].isna().any():\n",
    "            logger.warning(f\"Unmapped labels found, filling with 0\")\n",
    "            df['label'] = df['label'].fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = load_and_clean_dataset(train_file)\n",
    "val_df = load_and_clean_dataset(validation_file)\n",
    "test_df = load_and_clean_dataset(test_file, is_test=True)\n",
    "\n",
    "def augment_minority_classes(df, min_samples=500):\n",
    "    \"\"\"Simple augmentation by duplicating minority class samples with slight modifications\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    for label in df['label'].unique():\n",
    "        label_data = df[df['label'] == label]\n",
    "        current_count = len(label_data)\n",
    "        \n",
    "        if current_count < min_samples:\n",
    "           \n",
    "            needed = min_samples - current_count\n",
    "           \n",
    "            additional_samples = label_data.sample(n=needed, replace=True, random_state=42)\n",
    "            augmented_data.append(additional_samples)\n",
    "            logger.info(f\"Augmented label {label} from {current_count} to {min_samples} samples\")\n",
    "    \n",
    "    if augmented_data:\n",
    "        augmented_df = pd.concat([df] + augmented_data, ignore_index=True)\n",
    "        return augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "    return df\n",
    "\n",
    "\n",
    "original_train_size = len(train_df)\n",
    "train_df = augment_minority_classes(train_df)\n",
    "logger.info(f\"Training data augmented from {original_train_size} to {len(train_df)} samples\")\n",
    "\n",
    "\n",
    "logger.info(\"Train label distribution:\\n%s\", train_df['label'].value_counts().sort_index())\n",
    "logger.info(\"Validation label distribution:\\n%s\", val_df['label'].value_counts().sort_index())\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "model_name = 'csebuetnlp/banglabert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "\n",
    "max_seq_length = 384\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names \n",
    "                                            if col not in ['input_ids', 'attention_mask', 'label']])\n",
    "val_dataset = val_dataset.remove_columns([col for col in val_dataset.column_names \n",
    "                                        if col not in ['input_ids', 'attention_mask', 'label']])\n",
    "\n",
    "\n",
    "test_columns_to_keep = ['input_ids', 'attention_mask', 'id']\n",
    "test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names \n",
    "                                          if col not in test_columns_to_keep])\n",
    "\n",
    "\n",
    "classes = np.unique(train_df['label'])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "class AdvancedHybridHateModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels, hidden_size=768, lstm_hidden=384, cnn_out=256):\n",
    "        super().__init__()\n",
    "        \n",
    "       \n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "      \n",
    "        self.cnn1 = nn.Conv1d(hidden_size, cnn_out, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(hidden_size, cnn_out, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(hidden_size, cnn_out, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=cnn_out * 3, \n",
    "            hidden_size=lstm_hidden, \n",
    "            num_layers=2,  \n",
    "            bidirectional=True, \n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=lstm_hidden * 2, \n",
    "            num_heads=8, \n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "      \n",
    "        self.layer_norm = nn.LayerNorm(lstm_hidden * 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "   \n",
    "        self.classifier1 = nn.Linear(lstm_hidden * 2, lstm_hidden)\n",
    "        self.classifier2 = nn.Linear(lstm_hidden, lstm_hidden // 2)\n",
    "        self.classifier3 = nn.Linear(lstm_hidden // 2, num_labels)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(lstm_hidden)\n",
    "        self.bn2 = nn.BatchNorm1d(lstm_hidden // 2)\n",
    "    \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        for module in [self.classifier1, self.classifier2, self.classifier3]:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "      \n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state  \n",
    "        \n",
    "       \n",
    "        sequence_output_transposed = sequence_output.permute(0, 2, 1)  \n",
    "        \n",
    "        cnn_out1 = F.relu(self.cnn1(sequence_output_transposed))\n",
    "        cnn_out2 = F.relu(self.cnn2(sequence_output_transposed))\n",
    "        cnn_out3 = F.relu(self.cnn3(sequence_output_transposed))\n",
    "        \n",
    "\n",
    "        cnn_combined = torch.cat([cnn_out1, cnn_out2, cnn_out3], dim=1) \n",
    "        cnn_combined = cnn_combined.permute(0, 2, 1)  \n",
    "        \n",
    "   \n",
    "        lstm_out, _ = self.bilstm(cnn_combined)  \n",
    "        \n",
    "      \n",
    "        attn_out, _ = self.multihead_attn(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "  \n",
    "        lstm_out = self.layer_norm(lstm_out + attn_out)\n",
    "        \n",
    "      \n",
    "        max_pool = F.adaptive_max_pool1d(lstm_out.permute(0, 2, 1), 1).squeeze(-1)\n",
    "        avg_pool = F.adaptive_avg_pool1d(lstm_out.permute(0, 2, 1), 1).squeeze(-1)\n",
    "       \n",
    "        combined = max_pool + avg_pool\n",
    "        combined = self.dropout(combined)\n",
    "    \n",
    "        x = F.relu(self.bn1(self.classifier1(combined)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.classifier2(x)))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier3(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "     \n",
    "            alpha = 0.25\n",
    "            gamma = 2.0\n",
    "            ce_loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "            pt = torch.exp(-ce_loss)\n",
    "            focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "            loss = focal_loss.mean()\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss} if loss is not None else {'logits': logits}\n",
    "\n",
    "model = AdvancedHybridHateModel(model_name, num_labels)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "logger.info(f\"Model loaded on {device}\")\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    f1_weighted = f1_score(labels, preds, average=\"weighted\")\n",
    "    \n",
    " \n",
    "    f1_per_class = f1_score(labels, preds, average=None)\n",
    "    \n",
    "    result = {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "    }\n",
    "  \n",
    "    for i, f1 in enumerate(f1_per_class):\n",
    "        result[f\"f1_class_{i}\"] = f1\n",
    "    \n",
    "    return result\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./advanced_banglabert_hate_speech\",\n",
    "    learning_rate=2e-5,  \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1, \n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",  \n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    dataloader_drop_last=False,\n",
    "    gradient_accumulation_steps=2, \n",
    "    fp16=True,  \n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "class AdvancedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        \"\"\"Custom optimizer with different learning rates for different layers\"\"\"\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() \n",
    "                          if not any(nd in n for nd in no_decay) and \"base_model\" in n],\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "                \"lr\": self.args.learning_rate * 0.1,  \n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() \n",
    "                          if any(nd in n for nd in no_decay) and \"base_model\" in n],\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"lr\": self.args.learning_rate * 0.1,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() \n",
    "                          if not any(nd in n for nd in no_decay) and \"base_model\" not in n],\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "                \"lr\": self.args.learning_rate,  \n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() \n",
    "                          if any(nd in n for nd in no_decay) and \"base_model\" not in n],\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"lr\": self.args.learning_rate,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "       \n",
    "        optimizer_grouped_parameters = [group for group in optimizer_grouped_parameters if len(list(group[\"params\"])) > 0]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
    "        self.optimizer = optimizer\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "trainer = AdvancedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "logger.info(\"Starting training with enhanced model...\")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./advanced_banglabert_hate_speech\")\n",
    "\n",
    "\n",
    "logger.info(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(\"Validation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        logger.info(f\"{key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "val_predictions = trainer.predict(val_dataset)\n",
    "val_preds = np.argmax(val_predictions.predictions, axis=1)\n",
    "val_labels = val_dataset['label']\n",
    "\n",
    "logger.info(\"\\nValidation Classification Report:\")\n",
    "logger.info(\"\\n\" + classification_report(val_labels, val_preds, \n",
    "                                       target_names=list(hate_l2id.keys()), \n",
    "                                       digits=4))\n",
    "\n",
    "\n",
    "logger.info(\"Generating test predictions...\")\n",
    "\n",
    "test_prediction_dataset = test_dataset.remove_columns(['id'])\n",
    "test_predictions = trainer.predict(test_prediction_dataset)\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "output_file = \"./advanced_banglabert_hate_speech/subtask_1A.tsv\"\n",
    "os.makedirs(\"./advanced_banglabert_hate_speech\", exist_ok=True)\n",
    "\n",
    "with open(output_file, \"w\", encoding='utf-8') as writer:\n",
    "    writer.write(\"id\\tlabel\\tmodel\\n\")\n",
    "    for index, pred in enumerate(test_preds):\n",
    "        pred_label = id2hate[pred]\n",
    "        test_id = test_dataset['id'][index]\n",
    "        writer.write(f\"{test_id}\\t{pred_label}\\tadvanced-banglabert\\n\")\n",
    "\n",
    "logger.info(f\"Predictions saved to {output_file}\")\n",
    "logger.info(\"Training completed successfully!\")\n",
    "\n",
    "\n",
    "final_f1_micro = eval_results.get('eval_f1_micro', 0)\n",
    "logger.info(f\"\\nFinal F1-micro score: {final_f1_micro:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
