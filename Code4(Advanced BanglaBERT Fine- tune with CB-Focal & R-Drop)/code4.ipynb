{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da77792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.55.2\n",
      "PyTorch version: 2.7.1+cu126\n",
      "09/09/2025 04:42:48 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/09/2025 04:42:48 - WARNING - __main__ - Unmapped labels found, filling with 0\n",
      "09/09/2025 04:42:49 - INFO - __main__ - Original class distribution: {0: 23373, 1: 676, 2: 122, 3: 4227, 4: 2331, 5: 8212}\n",
      "09/09/2025 04:42:49 - INFO - __main__ - Training data: 38941 -> 60212 samples\n",
      "09/09/2025 04:42:49 - INFO - __main__ - Final train label distribution:\n",
      "label\n",
      "0    20000\n",
      "1     8000\n",
      "2     8000\n",
      "3     8000\n",
      "4     8000\n",
      "5     8212\n",
      "Name: count, dtype: int64\n",
      "09/09/2025 04:42:49 - INFO - __main__ - Validation label distribution:\n",
      "label\n",
      "0    1451\n",
      "1      38\n",
      "2      11\n",
      "3     291\n",
      "4     157\n",
      "5     564\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60212/60212 [00:04<00:00, 14131.25 examples/s]\n",
      "Map: 100%|██████████| 2512/2512 [00:00<00:00, 15478.21 examples/s]\n",
      "Map: 100%|██████████| 10200/10200 [00:00<00:00, 15651.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2025 04:42:55 - INFO - __main__ - CB-Focal class counts: {0: 20000, 1: 8000, 2: 8000, 3: 8000, 4: 8000, 5: 8212}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2025 04:42:56 - INFO - __main__ - Model loaded on cuda\n",
      "09/09/2025 04:42:56 - INFO - __main__ - Total parameters: 110,031,366\n",
      "09/09/2025 04:42:56 - INFO - __main__ - Trainable parameters: 110,031,366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2025 04:42:56 - INFO - __main__ - Starting research-optimized training for F1-micro 85%+...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510648\\AppData\\Local\\Temp\\ipykernel_14216\\1881434373.py:454: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLRDTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = LLRDTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='22584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6000/22584 45:17 < 2:05:14, 2.21 it/s, Epoch 3/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Class 0 None</th>\n",
       "      <th>F1 Class 1 Religious Hate</th>\n",
       "      <th>F1 Class 2 Sexism</th>\n",
       "      <th>F1 Class 3 Political Hate</th>\n",
       "      <th>F1 Class 4 Profane</th>\n",
       "      <th>F1 Class 5 Abusive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.204600</td>\n",
       "      <td>0.929318</td>\n",
       "      <td>0.519506</td>\n",
       "      <td>0.133098</td>\n",
       "      <td>0.411494</td>\n",
       "      <td>0.696380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.069498</td>\n",
       "      <td>0.010239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.102300</td>\n",
       "      <td>0.828088</td>\n",
       "      <td>0.545780</td>\n",
       "      <td>0.157016</td>\n",
       "      <td>0.440506</td>\n",
       "      <td>0.717103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.073314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.015200</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>0.566879</td>\n",
       "      <td>0.223910</td>\n",
       "      <td>0.472164</td>\n",
       "      <td>0.732519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.119171</td>\n",
       "      <td>0.408537</td>\n",
       "      <td>0.042414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.901900</td>\n",
       "      <td>0.620795</td>\n",
       "      <td>0.598726</td>\n",
       "      <td>0.308808</td>\n",
       "      <td>0.550308</td>\n",
       "      <td>0.764086</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.177778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.788100</td>\n",
       "      <td>0.513882</td>\n",
       "      <td>0.642118</td>\n",
       "      <td>0.386670</td>\n",
       "      <td>0.621325</td>\n",
       "      <td>0.788973</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.359465</td>\n",
       "      <td>0.644578</td>\n",
       "      <td>0.364253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.425963</td>\n",
       "      <td>0.664411</td>\n",
       "      <td>0.449813</td>\n",
       "      <td>0.661297</td>\n",
       "      <td>0.797814</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.442379</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.463661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.606200</td>\n",
       "      <td>0.413247</td>\n",
       "      <td>0.652070</td>\n",
       "      <td>0.482359</td>\n",
       "      <td>0.660798</td>\n",
       "      <td>0.777029</td>\n",
       "      <td>0.305344</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.462687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.386738</td>\n",
       "      <td>0.664809</td>\n",
       "      <td>0.489777</td>\n",
       "      <td>0.656479</td>\n",
       "      <td>0.801508</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.508861</td>\n",
       "      <td>0.708571</td>\n",
       "      <td>0.376068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.516200</td>\n",
       "      <td>0.351630</td>\n",
       "      <td>0.677150</td>\n",
       "      <td>0.516038</td>\n",
       "      <td>0.682639</td>\n",
       "      <td>0.795608</td>\n",
       "      <td>0.360902</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.538588</td>\n",
       "      <td>0.704871</td>\n",
       "      <td>0.491130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.357730</td>\n",
       "      <td>0.682325</td>\n",
       "      <td>0.501114</td>\n",
       "      <td>0.678445</td>\n",
       "      <td>0.806143</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.542816</td>\n",
       "      <td>0.696629</td>\n",
       "      <td>0.446953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.445200</td>\n",
       "      <td>0.349504</td>\n",
       "      <td>0.688296</td>\n",
       "      <td>0.512285</td>\n",
       "      <td>0.679515</td>\n",
       "      <td>0.809428</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.546565</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.439586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.326309</td>\n",
       "      <td>0.688694</td>\n",
       "      <td>0.522530</td>\n",
       "      <td>0.687859</td>\n",
       "      <td>0.799724</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.560132</td>\n",
       "      <td>0.724234</td>\n",
       "      <td>0.486540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>0.335188</td>\n",
       "      <td>0.678344</td>\n",
       "      <td>0.525781</td>\n",
       "      <td>0.683007</td>\n",
       "      <td>0.793855</td>\n",
       "      <td>0.378788</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.538860</td>\n",
       "      <td>0.744048</td>\n",
       "      <td>0.484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>0.319741</td>\n",
       "      <td>0.681927</td>\n",
       "      <td>0.540637</td>\n",
       "      <td>0.693302</td>\n",
       "      <td>0.783164</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.558875</td>\n",
       "      <td>0.726776</td>\n",
       "      <td>0.550198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.328257</td>\n",
       "      <td>0.666401</td>\n",
       "      <td>0.520949</td>\n",
       "      <td>0.675986</td>\n",
       "      <td>0.775037</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.693671</td>\n",
       "      <td>0.513680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.378700</td>\n",
       "      <td>0.305986</td>\n",
       "      <td>0.689092</td>\n",
       "      <td>0.555089</td>\n",
       "      <td>0.697772</td>\n",
       "      <td>0.786059</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.556114</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.556901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.308052</td>\n",
       "      <td>0.700637</td>\n",
       "      <td>0.546320</td>\n",
       "      <td>0.704104</td>\n",
       "      <td>0.809068</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.555891</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.526519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.326253</td>\n",
       "      <td>0.689889</td>\n",
       "      <td>0.527574</td>\n",
       "      <td>0.688481</td>\n",
       "      <td>0.802783</td>\n",
       "      <td>0.415385</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.552352</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.487234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.660828</td>\n",
       "      <td>0.521669</td>\n",
       "      <td>0.676879</td>\n",
       "      <td>0.765365</td>\n",
       "      <td>0.380368</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.543243</td>\n",
       "      <td>0.724719</td>\n",
       "      <td>0.534498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.305347</td>\n",
       "      <td>0.693073</td>\n",
       "      <td>0.550330</td>\n",
       "      <td>0.700883</td>\n",
       "      <td>0.791241</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.763848</td>\n",
       "      <td>0.553734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.306916</td>\n",
       "      <td>0.708599</td>\n",
       "      <td>0.556194</td>\n",
       "      <td>0.712342</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.571880</td>\n",
       "      <td>0.754286</td>\n",
       "      <td>0.554731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.309800</td>\n",
       "      <td>0.304275</td>\n",
       "      <td>0.692277</td>\n",
       "      <td>0.554747</td>\n",
       "      <td>0.702299</td>\n",
       "      <td>0.788104</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.767372</td>\n",
       "      <td>0.561199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.311629</td>\n",
       "      <td>0.696656</td>\n",
       "      <td>0.551670</td>\n",
       "      <td>0.702830</td>\n",
       "      <td>0.798999</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.563166</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.545966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>0.302237</td>\n",
       "      <td>0.712580</td>\n",
       "      <td>0.572774</td>\n",
       "      <td>0.712935</td>\n",
       "      <td>0.808849</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.572368</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.554194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.304791</td>\n",
       "      <td>0.709793</td>\n",
       "      <td>0.561990</td>\n",
       "      <td>0.706613</td>\n",
       "      <td>0.810333</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.565144</td>\n",
       "      <td>0.728863</td>\n",
       "      <td>0.532819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.289300</td>\n",
       "      <td>0.305419</td>\n",
       "      <td>0.701433</td>\n",
       "      <td>0.545309</td>\n",
       "      <td>0.703179</td>\n",
       "      <td>0.802812</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.566879</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.533835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.298375</td>\n",
       "      <td>0.705414</td>\n",
       "      <td>0.581391</td>\n",
       "      <td>0.711095</td>\n",
       "      <td>0.797675</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.566400</td>\n",
       "      <td>0.767908</td>\n",
       "      <td>0.571186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.302349</td>\n",
       "      <td>0.710987</td>\n",
       "      <td>0.566307</td>\n",
       "      <td>0.712370</td>\n",
       "      <td>0.810848</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.578871</td>\n",
       "      <td>0.754839</td>\n",
       "      <td>0.543842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.309071</td>\n",
       "      <td>0.705812</td>\n",
       "      <td>0.579972</td>\n",
       "      <td>0.707302</td>\n",
       "      <td>0.808421</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.551451</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.533195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.302763</td>\n",
       "      <td>0.708599</td>\n",
       "      <td>0.584597</td>\n",
       "      <td>0.715552</td>\n",
       "      <td>0.796337</td>\n",
       "      <td>0.466019</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.581583</td>\n",
       "      <td>0.779762</td>\n",
       "      <td>0.583882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2025 05:28:26 - INFO - __main__ - Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2025 05:28:38 - INFO - __main__ - Validation Results:\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_loss: 0.3022\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_micro: 0.7126\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_macro: 0.5728\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_weighted: 0.7129\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_class_0_None: 0.8088\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_class_1_Religious_Hate: 0.4324\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_class_2_Sexism: 0.3158\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_class_3_Political_Hate: 0.5724\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_class_4_Profane: 0.7530\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_f1_class_5_Abusive: 0.5542\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_runtime: 11.6712\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_samples_per_second: 215.2300\n",
      "09/09/2025 05:28:38 - INFO - __main__ - eval_steps_per_second: 6.7690\n",
      "09/09/2025 05:28:49 - INFO - __main__ - \n",
      "Validation Classification Report:\n",
      "09/09/2025 05:28:49 - INFO - __main__ - \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          None     0.8114    0.8063    0.8088      1451\n",
      "Religious Hate     0.3288    0.6316    0.4324        38\n",
      "        Sexism     0.3750    0.2727    0.3158        11\n",
      "Political Hate     0.5489    0.5979    0.5724       291\n",
      "       Profane     0.7143    0.7962    0.7530       157\n",
      "       Abusive     0.5915    0.5213    0.5542       564\n",
      "\n",
      "      accuracy                         0.7126      2512\n",
      "     macro avg     0.5616    0.6043    0.5728      2512\n",
      "  weighted avg     0.7163    0.7126    0.7129      2512\n",
      "\n",
      "09/09/2025 05:28:50 - INFO - __main__ - Generating test predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2025 05:29:10 - INFO - __main__ - Predictions saved to ./optimized_simple_banglabert/subtask_1A.tsv\n",
      "09/09/2025 05:29:10 - INFO - __main__ - \n",
      "🎯 Final Results:\n",
      "09/09/2025 05:29:10 - INFO - __main__ - F1-micro score: 0.7126\n",
      "09/09/2025 05:29:10 - INFO - __main__ - F1-macro score: 0.5728\n",
      "09/09/2025 05:29:10 - INFO - __main__ - 📈 Improvement achieved; consider ensembling or back-translation augmentation\n",
      "09/09/2025 05:29:10 - INFO - __main__ - Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Bangla Hate Speech Classification - Research-Grade Long-Tail Fixes\n",
    "#  CB-Focal + R-Drop + Multi-Sample Dropout + Mean Pooling + LLRD + Stronger Balancing\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "train_file = 'merged_dataset.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1A_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1A_test.tsv'\n",
    "\n",
    "def clean_bangla_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[।!?]{3,}', '।।', text)\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "hate_l2id = {'None': 0, 'Religious Hate': 1, 'Sexism': 2, 'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\n",
    "id2hate = {v: k for k, v in hate_l2id.items()}\n",
    "num_labels = len(hate_l2id)\n",
    "\n",
    "def load_and_clean_dataset(file_path, is_test=False):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df['text'] = df['text'].apply(clean_bangla_text)\n",
    "    df = df[df['text'].str.len() > 0]\n",
    "    if not is_test:\n",
    "        df['label'] = df['label'].map(hate_l2id)\n",
    "        if df['label'].isna().any():\n",
    "            logger.warning(f\"Unmapped labels found, filling with 0\")\n",
    "            df['label'] = df['label'].fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = load_and_clean_dataset(train_file)\n",
    "val_df = load_and_clean_dataset(validation_file)\n",
    "test_df = load_and_clean_dataset(test_file, is_test=True)\n",
    "\n",
    "def _rand_swap_words(words: List[str], n=1):\n",
    "    words = words[:]\n",
    "    for _ in range(n):\n",
    "        if len(words) < 2: break\n",
    "        i, j = random.sample(range(len(words)), 2)\n",
    "        words[i], words[j] = words[j], words[i]\n",
    "    return words\n",
    "\n",
    "def _rand_delete_words(words: List[str], p=0.1):\n",
    "    if len(words) <= 3: return words\n",
    "    return [w for w in words if random.random() > p] or words\n",
    "\n",
    "def _noisy_aug(text: str) -> str:\n",
    "    words = text.split()\n",
    "    if not words: return text\n",
    "\n",
    "    ops = []\n",
    "    if len(words) >= 6: ops.append('swap')\n",
    "    if len(words) >= 5: ops.append('delete')\n",
    "    if not ops: return text\n",
    "    for _ in range(random.choice([1, 2])):\n",
    "        op = random.choice(ops)\n",
    "        if op == 'swap':\n",
    "            words = _rand_swap_words(words, n=1)\n",
    "        elif op == 'delete':\n",
    "            words = _rand_delete_words(words, p=0.1)\n",
    "    out = ' '.join(words)\n",
    " \n",
    "    if random.random() < 0.3:\n",
    "        out = re.sub(r'([^\\w\\s])', r' \\1 ', out)\n",
    "        out = re.sub(r'\\s{2,}', ' ', out).strip()\n",
    "    return out\n",
    "\n",
    "def balanced_augmentation_strong(df: pd.DataFrame, cap_none=20000, target_per_class=6000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Cap 'None' at cap_none to reduce bias.\n",
    "    - Up-sample minorities with simple noise (swap/delete) until target_per_class (>= 6k) for all non-None classes.\n",
    "    \"\"\"\n",
    "    aug = []\n",
    "    class_counts = df['label'].value_counts().sort_index()\n",
    "    logger.info(f\"Original class distribution: {class_counts.to_dict()}\")\n",
    "    for label in sorted(df['label'].unique()):\n",
    "        cdf = df[df['label'] == label].copy()\n",
    "        if label == 0:\n",
    "\n",
    "            keep = min(len(cdf), cap_none)\n",
    "            cdf = cdf.sample(n=keep, random_state=42) if len(cdf) > keep else cdf\n",
    "            aug.extend(cdf.to_dict(orient='records'))\n",
    "        else:\n",
    "       \n",
    "            need = max(0, target_per_class - len(cdf))\n",
    "            if need > 0:\n",
    "                base_texts = cdf['text'].tolist()\n",
    "                for _ in range(need):\n",
    "                    t = random.choice(base_texts)\n",
    "                    aug.append({'text': _noisy_aug(t), 'label': label})\n",
    "           \n",
    "            aug.extend(cdf.to_dict(orient='records'))\n",
    "    res = pd.DataFrame(aug)\n",
    "    return res.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "original_train_size = len(train_df)\n",
    "train_df = balanced_augmentation_strong(train_df, cap_none=20000, target_per_class=8000)\n",
    "logger.info(f\"Training data: {original_train_size} -> {len(train_df)} samples\")\n",
    "logger.info(\"Final train label distribution:\\n%s\", train_df['label'].value_counts().sort_index())\n",
    "logger.info(\"Validation label distribution:\\n%s\", val_df['label'].value_counts().sort_index())\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "model_name = 'csebuetnlp/banglabert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "max_seq_length = 256  \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([c for c in train_dataset.column_names if c not in ['input_ids', 'attention_mask', 'label']])\n",
    "val_dataset = val_dataset.remove_columns([c for c in val_dataset.column_names if c not in ['input_ids', 'attention_mask', 'label']])\n",
    "test_dataset = test_dataset.remove_columns([c for c in test_dataset.column_names if c in ['text']]) \n",
    "\n",
    "class CBFocalLoss(nn.Module):\n",
    "   \n",
    "    def __init__(self, class_counts: torch.Tensor, beta: float = 0.9999, gamma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.register_buffer('alpha', self._compute_alpha(class_counts, beta))\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_alpha(class_counts: torch.Tensor, beta: float) -> torch.Tensor:\n",
    "        effective_num = 1.0 - torch.pow(torch.tensor(beta, dtype=torch.float, device=class_counts.device), class_counts.float())\n",
    "        weights = (1.0 - beta) / (effective_num + 1e-12)\n",
    "        weights = weights / weights.mean() \n",
    "        return weights.float()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        targets_onehot = F.one_hot(targets, num_classes=logits.size(-1)).float()\n",
    "\n",
    "        focal = torch.pow((1.0 - (probs * targets_onehot).sum(dim=-1)), self.gamma)\n",
    "\n",
    "        alpha_t = (self.alpha[targets]).to(logits.dtype)\n",
    "        ce = -(log_probs * targets_onehot).sum(dim=-1)\n",
    "        loss = alpha_t * focal * ce\n",
    "        return loss.mean()\n",
    "\n",
    "def kl_divergence_with_logits(p_logits, q_logits):\n",
    "    p = F.log_softmax(p_logits, dim=-1)\n",
    "    q = F.log_softmax(q_logits, dim=-1)\n",
    "    p_soft = p.exp()\n",
    "    q_soft = q.exp()\n",
    "    return (F.kl_div(p, q_soft, reduction='batchmean') + F.kl_div(q, p_soft, reduction='batchmean')) / 2.0\n",
    "\n",
    "class ResearchOptimizedClassifier(nn.Module):\n",
    "    def __init__(self, base_model_name: str, num_labels: int, class_counts: torch.Tensor,\n",
    "                 rdrop_alpha: float = 5.0, multi_sample: int = 4, dropout_p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        self.base_model.gradient_checkpointing_enable()\n",
    "        self.num_labels = num_labels\n",
    "        self.rdrop_alpha = rdrop_alpha\n",
    "        self.multi_sample = multi_sample\n",
    "\n",
    "        hidden = self.base_model.config.hidden_size\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(dropout_p) for _ in range(self.multi_sample)])\n",
    "        self.head = nn.Linear(hidden, num_labels)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "        self.cb_focal = CBFocalLoss(class_counts=class_counts.to(torch.float))\n",
    "\n",
    "    def _mean_pool(self, last_hidden_state, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        return summed / counts\n",
    "\n",
    "    def _logits_once(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self._mean_pool(outputs.last_hidden_state, attention_mask)\n",
    "\n",
    "        logits = 0\n",
    "        for dp in self.dropout_layers:\n",
    "            logits = logits + self.head(dp(pooled))\n",
    "        logits = logits / self.multi_sample\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        if labels is None:\n",
    "            logits = self._logits_once(input_ids, attention_mask)\n",
    "            return {'logits': logits}\n",
    " \n",
    "        logits1 = self._logits_once(input_ids, attention_mask)\n",
    "        logits2 = self._logits_once(input_ids, attention_mask)\n",
    "        ce1 = self.cb_focal(logits1, labels)\n",
    "        ce2 = self.cb_focal(logits2, labels)\n",
    "        kl = kl_divergence_with_logits(logits1, logits2)\n",
    "        loss = (ce1 + ce2) / 2.0 + self.rdrop_alpha * kl\n",
    "        return {'logits': (logits1 + logits2) / 2.0, 'loss': loss}\n",
    "\n",
    "train_counts_series = train_df['label'].value_counts().sort_index()\n",
    "class_counts_tensor = torch.tensor(train_counts_series.values, dtype=torch.float)\n",
    "logger.info(f\"CB-Focal class counts: {train_counts_series.to_dict()}\")\n",
    "\n",
    "model = ResearchOptimizedClassifier(\n",
    "    base_model_name=model_name,\n",
    "    num_labels=num_labels,\n",
    "    class_counts=class_counts_tensor,\n",
    "    rdrop_alpha=3.0,          \n",
    "    multi_sample=5,           \n",
    "    dropout_p=0.2\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "logger.info(f\"Model loaded on {device}\")\n",
    "logger.info(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "logger.info(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    f1_weighted = f1_score(labels, preds, average=\"weighted\")\n",
    "    f1_per_class = f1_score(labels, preds, average=None)\n",
    "    result = {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "    }\n",
    "    for i, f1v in enumerate(f1_per_class):\n",
    "        class_name = id2hate[i].replace(' ', '_')\n",
    "        result[f\"f1_class_{i}_{class_name}\"] = f1v\n",
    "    return result\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./optimized_simple_banglabert\",\n",
    "    learning_rate=2e-5,                 \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=12,                 \n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    dataloader_drop_last=False,\n",
    "    gradient_accumulation_steps=2,       \n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"cosine\",        \n",
    ")\n",
    "\n",
    "class LLRDTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is None:\n",
    "            base_lr = 1e-5\n",
    "            head_lr = 2e-4\n",
    "            weight_decay = self.args.weight_decay\n",
    "            layer_decay = 0.9\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\", \"layernorm.weight\"]\n",
    "\n",
    "            base_model = self.model.base_model\n",
    "            param_groups = []\n",
    "\n",
    "            if not (hasattr(base_model, \"encoder\") and hasattr(base_model.encoder, \"layer\")):\n",
    "                param_groups = [\n",
    "                    {\n",
    "                        \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": weight_decay,\n",
    "                        \"lr\": base_lr,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                        \"lr\": base_lr,\n",
    "                    },\n",
    "                ]\n",
    "            else:\n",
    "                layers = list(base_model.encoder.layer)\n",
    "                n = len(layers)\n",
    "\n",
    "\n",
    "                emb_params = list(base_model.embeddings.named_parameters())\n",
    "                lr = base_lr * (layer_decay ** n)\n",
    "                param_groups.append(\n",
    "                    {\n",
    "                        \"params\": [p for n_, p in emb_params if not any(nd in n_ for nd in no_decay)],\n",
    "                        \"weight_decay\": weight_decay,\n",
    "                        \"lr\": lr,\n",
    "                    }\n",
    "                )\n",
    "                param_groups.append(\n",
    "                    {\n",
    "                        \"params\": [p for n_, p in emb_params if any(nd in n_ for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                        \"lr\": lr,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "                for i, layer in enumerate(layers):\n",
    "                    depth = n - i - 1\n",
    "                    lr_i = base_lr * (layer_decay ** depth)\n",
    "                    named = list(layer.named_parameters())\n",
    "                    param_groups.append(\n",
    "                        {\n",
    "                            \"params\": [p for n_, p in named if not any(nd in n_ for nd in no_decay)],\n",
    "                            \"weight_decay\": weight_decay,\n",
    "                            \"lr\": lr_i,\n",
    "                        }\n",
    "                    )\n",
    "                    param_groups.append(\n",
    "                        {\n",
    "                            \"params\": [p for n_, p in named if any(nd in n_ for nd in no_decay)],\n",
    "                            \"weight_decay\": 0.0,\n",
    "                            \"lr\": lr_i,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                head_named = list(self.model.head.named_parameters())\n",
    "                param_groups.append(\n",
    "                    {\n",
    "                        \"params\": [p for n_, p in head_named if not any(nd in n_ for nd in no_decay)],\n",
    "                        \"weight_decay\": weight_decay,\n",
    "                        \"lr\": head_lr,\n",
    "                    }\n",
    "                )\n",
    "                param_groups.append(\n",
    "                    {\n",
    "                        \"params\": [p for n_, p in head_named if any(nd in n_ for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                        \"lr\": head_lr,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            self.optimizer = torch.optim.AdamW(param_groups, betas=(0.9, 0.999), eps=1e-8)\n",
    "        return self.optimizer\n",
    "\n",
    "    def create_scheduler(self, num_training_steps: int, optimizer=None):\n",
    "        if self.lr_scheduler is None:\n",
    "         \n",
    "            self.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer=self.optimizer,\n",
    "                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n",
    "                num_training_steps=num_training_steps,\n",
    "            )\n",
    "        return self.lr_scheduler\n",
    "\n",
    "trainer = LLRDTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=6)],\n",
    ")\n",
    "\n",
    "logger.info(\"Starting research-optimized training for F1-micro 85%+...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./optimized_simple_banglabert\")\n",
    "\n",
    "logger.info(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(\"Validation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        try:\n",
    "            logger.info(f\"{key}: {value:.4f}\")\n",
    "        except Exception:\n",
    "            logger.info(f\"{key}: {value}\")\n",
    "\n",
    "val_predictions = trainer.predict(val_dataset)\n",
    "val_preds = np.argmax(val_predictions.predictions, axis=1)\n",
    "val_labels = val_dataset['label']\n",
    "logger.info(\"\\nValidation Classification Report:\")\n",
    "report = classification_report(val_labels, val_preds, target_names=list(hate_l2id.keys()), digits=4)\n",
    "logger.info(\"\\n\" + report)\n",
    "\n",
    "\n",
    "logger.info(\"Generating test predictions...\")\n",
    "test_prediction_dataset = test_dataset.remove_columns(['id']) if 'id' in test_dataset.column_names else test_dataset\n",
    "test_predictions = trainer.predict(test_prediction_dataset)\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "os.makedirs(\"./optimized_simple_banglabert\", exist_ok=True)\n",
    "output_file = \"./optimized_simple_banglabert/subtask_1A.tsv\"\n",
    "with open(output_file, \"w\", encoding='utf-8') as writer:\n",
    "    writer.write(\"id\\tlabel\\tmodel\\n\")\n",
    "    for index, pred in enumerate(test_preds):\n",
    "        pred_label = id2hate[pred]\n",
    "        test_id = test_dataset['id'][index] if 'id' in test_dataset.column_names else index\n",
    "        writer.write(f\"{test_id}\\t{pred_label}\\toptimized-simple-banglabert\\n\")\n",
    "logger.info(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "\n",
    "final_f1_micro = eval_results.get('eval_f1_micro', 0)\n",
    "final_f1_macro = eval_results.get('eval_f1_macro', 0)\n",
    "logger.info(f\"\\n Final Results:\")\n",
    "logger.info(f\"F1-micro score: {final_f1_micro:.4f}\")\n",
    "logger.info(f\"F1-macro score: {final_f1_macro:.4f}\")\n",
    "\n",
    "logger.info(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
